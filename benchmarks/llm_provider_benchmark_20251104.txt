======================================================================
LLM Provider Benchmark Results - Ollama (Local)
======================================================================

Date: 2025-11-04
Test Environment: WSL2 Linux
Hardware: AMD Ryzen 9 9950X, 128GB RAM, RTX 4060 Ti 16GB
Ollama Version: Latest
Models Tested: qwen2.5:3b, qwen2.5:7b, qwen2.5:14b

======================================================================
Test Configuration
======================================================================

- Iterations per model: 4 (1st = warm-up, 2nd-4th = measurement)
- Warm-up run: DISCARDED from statistics
- Test prompt: "こんにちは"
- System prompt: "あなたは牡丹です。ギャル口調で話します。"
- Temperature: 0.7
- Max tokens: 100

======================================================================
Detailed Results
======================================================================

Model: qwen2.5:3b
------------------
Warm-up latency: 0.430s (discarded)
Measurement runs:
  Run 2: 0.291s
  Run 3: 0.450s
  Run 4: 0.362s

Statistics (Runs 2-4):
  Average:    0.368s
  Std Dev:    0.079s
  Min:        0.291s
  Max:        0.450s
  Cost:       $0.00 (local)

Model: qwen2.5:7b
------------------
Warm-up latency: 8.345s (discarded)
Measurement runs:
  Run 2: 0.726s
  Run 3: 0.409s
  Run 4: 1.317s

Statistics (Runs 2-4):
  Average:    0.817s
  Std Dev:    0.460s
  Min:        0.409s
  Max:        1.317s
  Cost:       $0.00 (local)

Model: qwen2.5:14b
------------------
Warm-up latency: 9.150s (discarded)
Measurement runs:
  Run 2: 1.958s
  Run 3: 1.550s
  Run 4: 0.990s

Statistics (Runs 2-4):
  Average:    1.499s
  Std Dev:    0.486s
  Min:        0.990s
  Max:        1.958s
  Cost:       $0.00 (local)

======================================================================
Summary
======================================================================

Model                Avg Latency     Std Dev      Min/Max             Cost
------------------------------------------------------------------------
qwen2.5:3b             0.368s        0.079s      0.291s / 0.450s    $0.00
qwen2.5:7b             0.817s        0.460s      0.409s / 1.317s    $0.00
qwen2.5:14b            1.499s        0.486s      0.990s / 1.958s    $0.00

======================================================================
Key Findings
======================================================================

1. Fastest Model: qwen2.5:3b (0.368s average)
   - Most stable (lowest standard deviation: 0.079s)
   - Ideal for lightweight, real-time interactions

2. Warm-up Impact:
   - 7b: 8.345s → 0.817s (10x speedup after warm-up)
   - 14b: 9.150s → 1.499s (6x speedup after warm-up)
   - Initial model loading adds 8-9 seconds

3. Performance Characteristics:
   - All models respond within 2 seconds after warm-up
   - Suitable for real-time conversational AI
   - Zero API cost (local execution)

4. Model Size vs Speed:
   - 3b: Smallest, fastest, most consistent
   - 7b: Good balance of speed and capability
   - 14b: Larger model, still sub-2s responses

======================================================================
Implementation Status
======================================================================

✅ LLM Provider Abstraction Layer - VERIFIED
✅ OllamaProvider Implementation - VERIFIED
✅ Response Generation - VERIFIED
✅ Latency Measurement - VERIFIED
✅ Cost Calculation - VERIFIED

======================================================================
Next Steps
======================================================================

- Test OpenAI API provider (gpt-4o-mini)
- Test Gemini API provider (gemini-1.5-flash)
- Compare local vs cloud LLM performance
- Write technical blog post for Qiita

======================================================================
