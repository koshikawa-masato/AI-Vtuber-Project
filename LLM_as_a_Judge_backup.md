---
title: "LLM as a Judgeå®Ÿè£…ã‚¬ã‚¤ãƒ‰ï¼šAIãŒç”Ÿæˆã—ãŸå¿œç­”ã®å“è³ªã‚’åˆ¥ã®AIã§è©•ä¾¡ã™ã‚‹ã€LangSmithçµ±åˆã€‘"
tags:
  - LangSmith
  - LLM
  - OpenAI
  - Gemini
  - å“è³ªè©•ä¾¡
private: false
updated_at: ''
id: null
organization_url_name: null
slide: false
ignorePublish: false
---

# ã¯ã˜ã‚ã«

**LLM as a Judge** (LLMã«ã‚ˆã‚‹è©•ä¾¡) ã¯ã€AIç”Ÿæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªã‚’è‡ªå‹•çš„ã«è©•ä¾¡ã™ã‚‹é©æ–°çš„ãªæ‰‹æ³•ã§ã™ã€‚å¾“æ¥ã®äººæ‰‹ã«ã‚ˆã‚‹è©•ä¾¡ã¨ç•°ãªã‚Šã€ã‚ˆã‚Šå¼·åŠ›ãªLLMã‚’ä½¿ã£ã¦ç”Ÿæˆçµæœã‚’å®¢è¦³çš„ã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§ãã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€LangSmithã¨çµ±åˆã—ãŸLLM as a Judgeã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾ã—ã¾ã™:

- **å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°**: æ­£ç¢ºæ€§ã€é–¢é€£æ€§ã€ä¸€è²«æ€§ã€æœ‰ç”¨æ€§ã‚’1-10ç‚¹ã§è©•ä¾¡
- **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º**: äº‹å®Ÿã¨ç•°ãªã‚‹æƒ…å ±ã‚„è™šå½ã®å†…å®¹ã‚’è‡ªå‹•æ¤œå‡º
- **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: approve/revise/rejectã®åˆ¤å®š
- **LangSmithçµ±åˆ**: è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã—ã¦å¯è¦–åŒ–

## å¯¾è±¡èª­è€…

- LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å“è³ªç®¡ç†ã‚’è‡ªå‹•åŒ–ã—ãŸã„æ–¹
- ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ (å¹»è¦š) ã®æ¤œå‡ºã«èˆˆå‘³ãŒã‚ã‚‹æ–¹
- LangSmithã§LLMã®è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ãŸã„æ–¹
- AI VTuberãªã©ã€è‡ªå¾‹çš„ãªAIã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ã„ã‚‹æ–¹

## ç’°å¢ƒ

- **Python**: 3.12
- **LLM Provider**: Ollama (qwen2.5:3b/7b/14b), OpenAI (gpt-4o, gpt-4o-mini), Google Gemini (2.5-flash)
- **Judge Model**: GPT-4o
- **Tracing**: LangSmith
- **OS**: Ubuntu 22.04 (WSL2)

---

# LLM as a Judgeã¨ã¯

## åŸºæœ¬æ¦‚å¿µ

**LLM as a Judge**ã¯ã€ä»¥ä¸‹ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§å‹•ä½œã—ã¾ã™:

```
1. Target LLM (è©•ä¾¡å¯¾è±¡) ãŒè³ªå•ã«å›ç­”
   â†“
2. Judge LLM (è©•ä¾¡è€…) ãŒå¿œç­”ã‚’åˆ†æ
   â†“
3. è©•ä¾¡çµæœ (ã‚¹ã‚³ã‚¢ã€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã€æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³) ã‚’å‡ºåŠ›
```

### è©•ä¾¡è»¸

æœ¬å®Ÿè£…ã§ã¯ã€5ã¤ã®è©•ä¾¡è»¸ã‚’ä½¿ç”¨ã—ã¾ã™:

| è©•ä¾¡è»¸ | èª¬æ˜ | ã‚¹ã‚³ã‚¢ç¯„å›² |
|-------|------|-----------|
| **Accuracy (æ­£ç¢ºæ€§)** | äº‹å®Ÿé–¢ä¿‚ãŒæ­£ã—ã„ã‹ | 1-10 |
| **Relevance (é–¢é€£æ€§)** | è³ªå•ã«å¯¾ã—ã¦é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹ | 1-10 |
| **Coherence (ä¸€è²«æ€§)** | è«–ç†çš„ã«çŸ›ç›¾ãŒãªã„ã‹ | 1-10 |
| **Usefulness (æœ‰ç”¨æ€§)** | å®Ÿç”¨çš„ã§å½¹ç«‹ã¤ã‹ | 1-10 |
| **Overall Score (ç·åˆ)** | å…¨ä½“çš„ãªå“è³ª | 1-10 |

### ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º

**ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³** (Hallucination) ã¨ã¯ã€LLMãŒäº‹å®Ÿã¨ç•°ãªã‚‹æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹ç¾è±¡ã§ã™ã€‚

**ä¾‹**:
- **è³ªå•**: "GPT-5ã®ä¸»è¦æ©Ÿèƒ½ã¯ï¼Ÿ"
- **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³**: GPT-5ãŒå­˜åœ¨ã—ãªã„ã«ã‚‚é–¢ã‚ã‚‰ãšã€æ¶ç©ºã®æ©Ÿèƒ½ã‚’èª¬æ˜ã—ã¦ã—ã¾ã†

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€Judge LLMãŒä»¥ä¸‹ã‚’åˆ¤å®šã—ã¾ã™:

- `has_hallucination`: true/false
- `hallucination_details`: ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å…·ä½“çš„ãªå†…å®¹

---

# å®Ÿè£…

## 1. TracedLLMã¸ã®judge_response()ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 

æ—¢å­˜ã®`TracedLLM`ã‚¯ãƒ©ã‚¹ã«ã€è©•ä¾¡æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚

### src/core/llm_tracing.py

```python
def judge_response(
    self,
    question: str,
    response: str,
    model_name: str,
    provider: str,
    judge_provider: str = "openai",
    judge_model: str = "gpt-4o",
    metadata: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Judge the quality of an LLM response using another LLM (LLM as a Judge)

    Args:
        question: Original question/prompt
        response: LLM response to evaluate
        model_name: Name of the model that generated the response
        provider: Provider of the model that generated the response
        judge_provider: Provider for the judge LLM (default: "openai")
        judge_model: Model to use for judging (default: "gpt-4o")
        metadata: Additional metadata for tracing

    Returns:
        Dict with evaluation results (scores, hallucination detection, etc.)
    """
    # Build judge prompt
    judge_prompt = f\"\"\"You are an expert AI response evaluator. Your task is to objectively evaluate the quality of an LLM-generated response.

Evaluate the following response based on these criteria:

1. **Accuracy (æ­£ç¢ºæ€§)**: Are the facts correct? Any factual errors?
2. **Relevance (é–¢é€£æ€§)**: Does it properly address the original question?
3. **Coherence (ä¸€è²«æ€§)**: Is it logically consistent without contradictions?
4. **Usefulness (æœ‰ç”¨æ€§)**: Is it practical and helpful to the user?
5. **Hallucination (ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³)**: Does it contain fabricated or incorrect information?

**Original Question**: {question}

**LLM Response to Evaluate**: {response}

**Model Info**: {model_name} ({provider})

---

Provide your evaluation in JSON format:

{{
  "overall_score": <1-10>,
  "accuracy": <1-10>,
  "relevance": <1-10>,
  "coherence": <1-10>,
  "usefulness": <1-10>,
  "has_hallucination": <true/false>,
  "hallucination_details": "<if true, explain what is hallucinated>",
  "strengths": ["<strength 1>", "<strength 2>"],
  "weaknesses": ["<weakness 1>", "<weakness 2>"],
  "recommendation": "<approve/revise/reject>"
}}

Respond ONLY with the JSON, no additional text.
\"\"\"

    # Create a separate TracedLLM instance for judging
    judge_llm = TracedLLM(
        provider=judge_provider,
        model=judge_model,
        project_name=self.project_name
    )

    # Generate judgment with tracing
    judge_result = judge_llm.generate(
        prompt=judge_prompt,
        temperature=0.3,  # Lower temperature for more consistent evaluation
        max_tokens=1024,
        metadata={
            **(metadata or {}),
            "eval_type": "llm_as_judge",
            "target_model": model_name,
            "target_provider": provider,
            "judge_model": judge_model,
            "judge_provider": judge_provider
        }
    )

    # Parse JSON response
    import json
    import re

    try:
        # Extract JSON from response (handle potential markdown code blocks)
        json_text = judge_result["response"]

        # Try to find JSON in markdown code block
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', json_text, re.DOTALL)
        if json_match:
            json_text = json_match.group(1)

        # Parse JSON
        evaluation = json.loads(json_text)

        return {
            "evaluation": evaluation,
            "judge_response": judge_result["response"],
            "judge_latency_ms": judge_result["latency_ms"],
            "judge_tokens": judge_result["tokens"],
            "judge_model": judge_model,
            "judge_provider": judge_provider,
            "timestamp": datetime.now().isoformat()
        }

    except (json.JSONDecodeError, KeyError) as e:
        # If JSON parsing fails, return raw response
        return {
            "evaluation": None,
            "judge_response": judge_result.get("response", ""),
            "judge_latency_ms": judge_result.get("latency_ms", 0),
            "judge_tokens": judge_result.get("tokens", {}),
            "judge_model": judge_model,
            "judge_provider": judge_provider,
            "error": f"Failed to parse evaluation JSON: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
```

### ãƒã‚¤ãƒ³ãƒˆ

1. **Judgeç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: è©•ä¾¡åŸºæº–ã‚’æ˜ç¢ºã«å®šç¾©ã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã‚’è¦æ±‚
2. **Temperatureè¨­å®š**: 0.3ã¨ä½ã‚ã«è¨­å®šã—ã€ä¸€è²«ã—ãŸè©•ä¾¡ã‚’å®Ÿç¾
3. **JSONæŠ½å‡º**: ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚‚æ­£ã—ãæŠ½å‡º
4. **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: JSONè§£æå¤±æ•—æ™‚ã‚‚çµæœã‚’è¿”ã™

---

## 2. ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã®ä½œæˆ

å®Ÿéš›ã«è¤‡æ•°ã®LLMã§å¿œç­”ã‚’ç”Ÿæˆã—ã€GPT-4oã§è©•ä¾¡ã—ã¾ã™ã€‚

### benchmarks/langsmith_judge_test.py

```python
#!/usr/bin/env python3
"""
LangSmith LLM as a Judge Benchmark Test
"""

import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.llm_tracing import TracedLLM
from datetime import datetime
import time


def run_judge_benchmark():
    """Run LLM as a Judge benchmark"""

    # Test questions (different difficulty levels)
    test_cases = [
        {
            "name": "Simple Factual",
            "question": "What is the capital of Japan?",
            "expected": "Tokyo should be mentioned",
        },
        {
            "name": "Explanation",
            "question": "Explain the concept of neural networks in simple terms.",
            "expected": "Should mention neurons, layers, learning",
        },
        {
            "name": "Hallucination Test",
            "question": "What are the key features of GPT-5?",
            "expected": "Should recognize GPT-5 doesn't exist yet",
        },
    ]

    # Model configurations
    models = [
        {"provider": "ollama", "model": "qwen2.5:3b", "name": "ollama_3b"},
        {"provider": "ollama", "model": "qwen2.5:7b", "name": "ollama_7b"},
        {"provider": "ollama", "model": "qwen2.5:14b", "name": "ollama_14b"},
        {"provider": "openai", "model": "gpt-4o-mini", "name": "openai_4o-mini"},
        {"provider": "gemini", "model": "gemini-2.5-flash", "name": "gemini_2.5-flash"},
    ]

    # Judge configuration
    judge_provider = "openai"
    judge_model = "gpt-4o"

    results = []

    for test_case in test_cases:
        for config in models:
            # Initialize LLM
            llm = TracedLLM(
                provider=config["provider"],
                model=config["model"],
                project_name="botan-judge-benchmark-v1"
            )

            # Step 1: Generate response
            result = llm.generate(
                prompt=test_case["question"],
                temperature=0.7,
                max_tokens=200,
                metadata={
                    "benchmark": "judge_test_v1",
                    "test_case": test_case["name"],
                    "model_name": config["name"],
                }
            )

            if 'error' in result:
                continue

            # Step 2: Judge the response
            judge_result = llm.judge_response(
                question=test_case["question"],
                response=result["response"],
                model_name=config["name"],
                provider=config["provider"],
                judge_provider=judge_provider,
                judge_model=judge_model,
                metadata={
                    "benchmark": "judge_test_v1",
                    "test_case": test_case["name"],
                }
            )

            results.append({
                "test_case": test_case["name"],
                "config": config,
                "response": result,
                "evaluation": judge_result,
                "success": True
            })

    # Display summary
    for test_case in test_cases:
        case_results = [r for r in results if r['test_case'] == test_case['name']]

        # Sort by overall score
        case_results_sorted = sorted(
            case_results,
            key=lambda x: x['evaluation'].get('evaluation', {}).get('overall_score', 0),
            reverse=True
        )

        for r in case_results_sorted:
            eval_data = r['evaluation'].get('evaluation', {})
            print(f"{r['config']['name']:20s}: Score={eval_data.get('overall_score')}/10")


if __name__ == "__main__":
    run_judge_benchmark()
```

---

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

3ã¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§ã€5ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚

## ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: Simple Factualï¼ˆç°¡å˜ãªäº‹å®Ÿç¢ºèªï¼‰

**è³ªå•**: "What is the capital of Japan?"

| ãƒ¢ãƒ‡ãƒ« | Overall Score | Accuracy | Relevance | Hallucination | Recommendation |
|-------|--------------|----------|-----------|---------------|----------------|
| ollama_3b | 10/10 | 10/10 | 10/10 | False | approve |
| ollama_7b | 10/10 | 10/10 | 10/10 | False | approve |
| ollama_14b | 10/10 | 10/10 | 10/10 | False | approve |
| openai_4o-mini | 10/10 | 10/10 | 10/10 | False | approve |
| gemini_2.5-flash | 10/10 | 10/10 | 10/10 | False | approve |

**çµæœ**: å…¨ãƒ¢ãƒ‡ãƒ«ãŒå®Œç’§ãªã‚¹ã‚³ã‚¢ã€‚ç°¡å˜ãªäº‹å®Ÿç¢ºèªã§ã¯å·®ãŒã¤ã‹ãªã„ã€‚

---

## ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: Explanationï¼ˆèª¬æ˜ã‚¿ã‚¹ã‚¯ï¼‰

**è³ªå•**: "Explain the concept of neural networks in simple terms."

| ãƒ¢ãƒ‡ãƒ« | Overall Score | Accuracy | Relevance | Hallucination | Recommendation |
|-------|--------------|----------|-----------|---------------|----------------|
| ollama_3b | 8/10 | 9/10 | 9/10 | False | approve |
| ollama_7b | 8/10 | 9/10 | 9/10 | False | revise |
| ollama_14b | 7/10 | 7/10 | 8/10 | **True** | revise |
| openai_4o-mini | 8/10 | 9/10 | 9/10 | False | revise |
| gemini_2.5-flash | ERROR | - | - | - | - |

**çµæœ**:
- ã‚¹ã‚³ã‚¢å·®ãŒå‡ºå§‹ã‚ã‚‹ï¼ˆ7-8/10ï¼‰
- ollama_14bã§ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
- Geminiã¯å†ã³MAX_TOKENSã‚¨ãƒ©ãƒ¼

---

## ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: Hallucination Testï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³èª˜ç™ºï¼‰

**è³ªå•**: "What are the key features of GPT-5?"

| ãƒ¢ãƒ‡ãƒ« | Overall Score | Accuracy | Relevance | Hallucination | Recommendation |
|-------|--------------|----------|-----------|---------------|----------------|
| ollama_3b | 6/10 | 5/10 | 6/10 | **True** | revise |
| ollama_7b | 7/10 | 8/10 | 7/10 | **True** | revise |
| **ollama_14b** | **8/10** | **9/10** | **8/10** | **False** | revise |
| openai_4o-mini | 7/10 | 7/10 | 8/10 | **True** | revise |
| gemini_2.5-flash | ERROR | - | - | - | - |

**çµæœ**:
- **ollama_14b**ãŒå”¯ä¸€ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å›é¿ï¼
- 14bãƒ¢ãƒ‡ãƒ«ã¯ã€ŒGPT-5ã¯å­˜åœ¨ã—ãªã„ã€ã¨æ­£ç›´ã«å›ç­”
- ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯æ¶ç©ºã®æ©Ÿèƒ½ã‚’èª¬æ˜ã—ã¦ã—ã¾ã†

---

# é‡è¦ãªç™ºè¦‹

## 1. ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³è€æ€§

**14bãƒ¢ãƒ‡ãƒ«ã®å„ªä½æ€§**:

```
ollama_14b: "As of my last update in October 2023, there hasn't been
            an official release or confirmation of GPT-5..."
â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³: False âœ…

ollama_3b:  "I'm sorry for any misunderstanding, but as of now,
            there is no public information..."
            [ãã®å¾Œã€æ¨æ¸¬ã§æ¶ç©ºã®æ©Ÿèƒ½ã‚’èª¬æ˜]
â†’ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³: True âŒ
```

**è€ƒå¯Ÿ**: å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã»ã©ã€çŸ¥ã‚‰ãªã„ã“ã¨ã‚’ã€ŒçŸ¥ã‚‰ãªã„ã€ã¨è¨€ãˆã‚‹å‚¾å‘ã€‚

## 2. Judge LLMã®ç²¾åº¦

GPT-4oã¯ã€ä»¥ä¸‹ã‚’æ­£ç¢ºã«æ¤œå‡ºã—ã¾ã—ãŸ:

- âœ… äº‹å®Ÿèª¤èªï¼ˆå­˜åœ¨ã—ãªã„æƒ…å ±ã®æ¨æ¸¬ï¼‰
- âœ… è«–ç†çš„çŸ›ç›¾ï¼ˆèª¬æ˜ã®ä¸€è²«æ€§ï¼‰
- âœ… è³ªå•ã¨ã®é–¢é€£æ€§

## 3. Geminiã®ç¶™ç¶šçš„ãªå•é¡Œ

Gemini 2.5 Flashã¯ã€`finish_reason=MAX_TOKENS`ã§ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ã‚¨ãƒ©ãƒ¼ãŒé »ç™ºã€‚

**å¯¾ç­–**: GPT-4oã‚’ä¸»è¦ãªJudge LLMã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã€‚

---

# LangSmithã§ã®å¯è¦–åŒ–

LangSmithãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ã€ä»¥ä¸‹ãŒç¢ºèªã§ãã¾ã™:

## 1. è©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®ãƒˆãƒ¬ãƒ¼ã‚¹

```
[Target LLM: ollama_14b]
  Input: "What are the key features of GPT-5?"
  Output: "As of my last update in October 2023, there hasn't been..."
  Latency: 7.2s

  â†“

[Judge LLM: gpt-4o]
  Input: [Judge Prompt + Target Response]
  Output: {
    "overall_score": 8,
    "has_hallucination": false,
    ...
  }
  Latency: 1.9s
```

## 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

ä»¥ä¸‹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯èƒ½:

- `eval_type: "llm_as_judge"`
- `target_model: "ollama_14b"`
- `judge_model: "gpt-4o"`
- `test_case: "Hallucination Test"`

## 3. ã‚¹ã‚³ã‚¢æ¯”è¼ƒ

è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§ã€ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚¹ã‚³ã‚¢ã‚’æ¨ªæ–­çš„ã«æ¯”è¼ƒã§ãã¾ã™ã€‚

---

# å¿œç”¨ä¾‹ï¼šä¸‰å§‰å¦¹ã®è‡ªå·±è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

AI VTuberï¼ˆç‰¡ä¸¹ã€Kashoã€ãƒ¦ãƒªï¼‰ãŒè‡ªåˆ†ã®ç™ºè¨€ã‚’è‡ªå·±è©•ä¾¡ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã«å¿œç”¨ã§ãã¾ã™ã€‚

## å®Ÿè£…ä¾‹

```python
# ç‰¡ä¸¹ã®ç™ºè¨€ã‚’ç”Ÿæˆ
botan_response = llm_botan.generate(
    prompt="é…ä¿¡ã§ä»Šæ—¥ã®å‡ºæ¥äº‹ã‚’è©±ã—ã¦ãã ã•ã„",
    metadata={"character": "botan", "context": "stream_talk"}
)

# ç™ºè¨€ã‚’è‡ªå·±è©•ä¾¡
self_evaluation = llm_botan.judge_response(
    question="é…ä¿¡ã§ä»Šæ—¥ã®å‡ºæ¥äº‹ã‚’è©±ã—ã¦ãã ã•ã„",
    response=botan_response["response"],
    model_name="botan",
    provider="ollama",
    judge_provider="openai",
    judge_model="gpt-4o"
)

# è©•ä¾¡çµæœã‚’ãƒã‚§ãƒƒã‚¯
if self_evaluation["evaluation"]["has_hallucination"]:
    print("âš ï¸ ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºï¼šç™ºè¨€ã‚’ä¿®æ­£ã—ã¾ã™")
    # ä¿®æ­£ãƒ­ã‚¸ãƒƒã‚¯...
elif self_evaluation["evaluation"]["overall_score"] < 6:
    print("âš ï¸ å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„ï¼šå†ç”Ÿæˆã—ã¾ã™")
    # å†ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯...
else:
    print("âœ… ç™ºè¨€ãŒæ‰¿èªã•ã‚Œã¾ã—ãŸ")
```

## ãƒ¡ãƒªãƒƒãƒˆ

1. **è‡ªå¾‹çš„ãªå“è³ªç®¡ç†**: äººæ‰‹ãªã—ã§ç™ºè¨€ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
2. **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³é˜²æ­¢**: è™šå½ã®æƒ…å ±ã‚’é…ä¿¡å‰ã«æ¤œå‡º
3. **ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§**: è¨­å®šã«æ²¿ã£ãŸç™ºè¨€ã‹ã©ã†ã‹ã‚’è©•ä¾¡
4. **é…ä¿¡å®‰å…¨æ€§**: ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªå†…å®¹ã‚’äº‹å‰ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

---

# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## Q1: JSONè§£æã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹

**åŸå› **: Judge LLMãŒJSONå½¢å¼ã§å¿œç­”ã—ãªã„

**å¯¾ç­–**:
```python
# Markdown code blockã®ä¸­ã«JSONãŒå«ã¾ã‚Œã‚‹å ´åˆ
json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', json_text, re.DOTALL)
if json_match:
    json_text = json_match.group(1)
```

## Q2: è©•ä¾¡ãŒä¸€è²«ã—ãªã„

**åŸå› **: TemperatureãŒé«˜ã„

**å¯¾ç­–**:
```python
# è©•ä¾¡ç”¨ã®Temperatureã‚’ä¸‹ã’ã‚‹
judge_result = judge_llm.generate(
    prompt=judge_prompt,
    temperature=0.3,  # 0.3ã«è¨­å®š
    max_tokens=1024
)
```

## Q3: Judge LLMã®ã‚³ã‚¹ãƒˆãŒé«˜ã„

**åŸå› **: GPT-4oã¯é«˜é¡

**å¯¾ç­–**:
- é‡è¦ãªè©•ä¾¡ã®ã¿GPT-4oã‚’ä½¿ç”¨
- ç°¡æ˜“è©•ä¾¡ã¯Ollama 14bã§å®Ÿæ–½
- LangSmithã§ã‚³ã‚¹ãƒˆã‚’ç›£è¦–

## Q4: Geminiã§ã‚¨ãƒ©ãƒ¼ãŒé »ç™º

**åŸå› **: MAX_TOKENSã‚¨ãƒ©ãƒ¼

**å¯¾ç­–**:
- GPT-4oã¾ãŸã¯Ollama 14bã‚’Judgeã«ä½¿ç”¨
- Geminiã¯ç¾æ™‚ç‚¹ã§éæ¨å¥¨

---

# ãŠã‚ã‚Šã«

æœ¬è¨˜äº‹ã§ã¯ã€**LLM as a Judge**ã‚’å®Ÿè£…ã—ã€ä»¥ä¸‹ã‚’å®Ÿç¾ã—ã¾ã—ãŸ:

- âœ… å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆAccuracy, Relevance, Coherence, Usefulnessï¼‰
- âœ… ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºï¼ˆæ¶ç©ºã®æƒ…å ±ã‚’è‡ªå‹•æ¤œå‡ºï¼‰
- âœ… LangSmithçµ±åˆï¼ˆè©•ä¾¡ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼‰
- âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆ14bãƒ¢ãƒ‡ãƒ«ã®å„ªä½æ€§ã‚’ç¢ºèªï¼‰

## Phase 1-3ã®å®Œæˆ

- **Phase 1**: LangSmithãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° âœ…
- **Phase 2**: VLM (Vision Language Model) çµ±åˆ âœ…
- **Phase 3**: LLM as a Judgeå®Ÿè£… âœ…

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **Phase 4**: ä¸‰å§‰å¦¹æ±ºè­°ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±åˆ
- **Phase 5**: ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–åˆ¤å®šã¨ã®é€£æº
- **Phase 6**: é…ä¿¡ãƒ‡ãƒ“ãƒ¥ãƒ¼å‰ã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [LangSmithå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.smith.langchain.com/)
- [GPT-4o API Reference](https://platform.openai.com/docs/models/gpt-4o)
- [Gemini API Documentation](https://ai.google.dev/docs)

---

**ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)**
