---
title: OpenAIのgpt-oss-120bをOllamaで試してみた【速報】
tags:
  - AI
  - LLM
  - OpenAI
  - Ollama
  - gpt-oss
private: false
updated_at: ''
id: null
organization_url_name: null
slide: false
ignorePublish: false
---

## はじめに

OpenAIが2025年8月にオープンウェイトモデル「gpt-oss」をリリースしました。今回は120Bパラメータ版の `gpt-oss:120b` をOllamaで試してみたので、速報的にレポートします。

## gpt-oss-120bの特徴

### 基本スペック

- **パラメータ数**: 117B（実際のアクティブは5.1B/トークン）
- **アーキテクチャ**: Mixture-of-Experts (MoE)
- **ライセンス**: Apache 2.0（商用利用可能）
- **動作環境**: 単一80GB GPU（H100, MI300Xなど）

### 際立った特徴

1. **推論過程の可視化**
   - Chain-of-Thoughtによる思考プロセスが見える
   - デバッグやトラストが容易

2. **推論レベルの調整**
   - low / medium / high の3段階
   - システムメッセージで簡単に設定可能

3. **エージェント機能**
   - 関数呼び出し
   - Web検索（Ollamaが対応予定）
   - Python実行
   - 構造化出力

## Ollamaで試してみた

### インストール

```bash
ollama pull gpt-oss:120b
```

ダウンロードサイズは約65GB。10GbE環境で約6分でダウンロード完了しました。

### 実際に使ってみた

自作のAI VTuberチャットシステム「コピーロボット」で、キャラクター「牡丹」との会話に使ってみました。

![gpt-oss-120bでの会話例](https://github.com/koshikawa-masato/AI-Vtuber-Project/blob/main/screenshots/run_copy_robot_gpt-oss-120b.jpg?raw=true)

### 思考プロセスのフィルタリング

gpt-oss-120bは以下の形式で出力します：

```
Thinking...
[内部思考プロセス]
...done thinking.

[実際の応答]
```

この「Thinking...」部分は開発には有用ですが、エンドユーザーには不要なので、以下のようにフィルタリングしました：

```python
# 出力から思考部分を除去
if response.startswith('Thinking...'):
    thinking_end = response.find('...done thinking.')
    if thinking_end != -1:
        response = response[thinking_end + len('...done thinking.'):].strip()
```

### 応答速度

- **環境**: WSL2 Ubuntu 24.04
- **応答時間**: 12〜46秒/応答
- **推論速度**: 約8.5 tokens/s

120Bモデルとしては妥当な速度です。より速い応答が必要な場合は `gpt-oss:20b` も選択肢になります。

## これまでのモデルとの比較

自分のAI VTuberプロジェクトでは、これまで **qwen2.5:14b** を中心に使ってきました。gpt-oss:120bと比較してみます。

### qwen2.5:14b（リアルタイム応答重視）

**特徴**
- パラメータ数: 14B
- 応答速度: 2〜5秒/応答
- メモリ使用量: 約9GB
- 用途: リアルタイム会話、配信応答

**メリット**
- ⚡ レスポンスが速い（ユーザーを待たせない）
- 💾 メモリ消費が少ない（複数モデル同時起動可能）
- 🎯 カジュアルな会話には十分な品質

**デメリット**
- 複雑な推論タスクには限界
- 長文生成の一貫性がやや弱い
- ツール呼び出し機能は限定的

### gpt-oss:120b（推論重視）

**特徴**
- パラメータ数: 117B（アクティブ5.1B）
- 応答速度: 12〜46秒/応答
- メモリ使用量: 約65GB
- 用途: 複雑な推論、エージェント処理

**メリット**
- 🧠 推論過程が可視化される（Chain-of-Thought）
- 🔧 ツール利用・関数呼び出しが強力
- 📊 複雑な問題解決能力が高い
- 📝 長文生成の品質・一貫性が高い

**デメリット**
- 🐌 応答に時間がかかる
- 💾 メモリ使用量が大きい
- 🔥 発熱・消費電力が大きい

### 使い分けの提案

| 用途 | 推奨モデル | 理由 |
|------|-----------|------|
| リアルタイム会話 | qwen2.5:14b | 応答速度が重要 |
| ライブ配信応答 | qwen2.5:14b | ユーザーを待たせない |
| 複雑な質問応答 | gpt-oss:120b | 推論能力が必要 |
| コード生成・デバッグ | gpt-oss:120b | 論理的思考が重要 |
| エージェント処理 | gpt-oss:120b | ツール利用が必要 |
| バッチ処理 | gpt-oss:120b | 時間制約が緩い |

### ハイブリッド運用の可能性

実際のAI VTuberプロジェクトでは、以下のようなハイブリッド運用が考えられます：

```
[ユーザー入力]
    ↓
[軽量フィルタ（qwen2.5:14b）]
    ↓ 簡単な質問
[即座に応答（2-5秒）]
    ↓ 複雑な質問
[gpt-oss:120bで深い推論（20-40秒）]
    ↓
[高品質な応答]
```

このアプローチにより、多くの質問には素早く応答しつつ、必要な時だけ深い推論を行うことができます。

### 実際の比較テスト

同じ複雑な質問「AI VTuberが記憶を持つシステムを作りたいのですが、RAGとベクトルデータベースを使う方法と、イベントベースの記憶システムを使う方法のどちらが良いと思いますか？それぞれのメリット・デメリットを簡潔に教えてください。」を両モデルに投げてみました。

#### qwen2.5:14bの結果

**応答時間**: 28.9秒

**実際の応答（抜粋）**:

> AI VTuberの記憶システムについて、RAG+ベクトルデータベースとイベントベースのアプローチを比較します。
>
> **RAG（Retrieval-Augmented Generation）+ ベクトルデータベース**
>
> メリット:
> - セマンティック検索が可能で、似た文脈の情報を効率的に取得
> - 大規模なデータセットでも高速に検索可能
> - LLMと自然に統合しやすく、文脈に応じた応答生成に適している
>
> デメリット:
> - ベクトル化のための計算コストがかかる
> - リアルタイム性がやや劣る（埋め込み生成に時間）
> - データの更新・削除が複雑
>
> **イベントベース記憶システム**
>
> メリット:
> - 時系列データの管理が容易で、「いつ・何が起きたか」を明確に記録
> - リアルタイム性が高く、即座にデータを追加・参照可能
> - シンプルな実装で、データの更新・削除も簡単
>
> デメリット:
> - セマンティック検索が難しく、キーワードベースの検索に限定される
> - 大規模データになると検索速度が低下する可能性
> - 文脈理解が弱く、関連性の高い情報を見つけにくい
>
> **推奨**:
> - リアルタイム性と時系列管理が重要 → イベントベース
> - 文脈理解とセマンティック検索が重要 → RAG+ベクトルDB
> - または両者のハイブリッド運用も有効

**応答の特徴**:
- 簡潔で読みやすい比較
- RAGとイベントベースの基本的なメリット・デメリットを網羅
- 実用的な推奨事項を提示
- 全体で約500〜600語

#### gpt-oss:120bの結果

**応答時間**: 2分以上

**実際の応答（抜粋）**:

> ## 1. RAG（Retrieval‑Augmented Generation）＋ベクトルデータベース方式
>
> | 項目 | メリット | デメリット |
> |------|----------|-------------|
> | **検索精度・汎用性** | ・埋め込みベクトルで意味的に類似した過去発言・シナリオを高速に検索できる。<br>・新しいトピックや予期しない質問にも、過去の類似コンテキストを引き出せる。 | ・埋め込みが「意味」だけを捉えるため、**時間的な順序**や「誰が言ったか」などのメタ情報は別途管理が必要。 |
> | **スケーラビリティ** | ・インデックス（FAISS / Qdrant / Milvus など）で数十万～数百万件でもミリ秒検索が可能。 | ・インデックス作成・更新コストがかかる。大量データが増えるとストレージとメモリの要件が上がる。 |
> | **実装のハードル** | ・既存ライブラリが豊富（LangChain, LlamaIndex, Haystack 等）。<br>・LLM とのインタフェースがシンプル（「検索 → プロンプトに埋め込む」だけ）。 | ・ベクトル化・インデックス更新のパイプラインを自前で構築しないと、**リアルタイム性**が低くなることがある。 |
>
> ### 典型的な構成例
> 1. 発言・イベント → テキスト前処理 → **埋め込み**（Sentence‑Transformer / OpenAI embeddings）
> 2. ベクトルDBに **INSERT**（メタ情報：timestamp, speaker, scene など）
> 3. ユーザー発話 → 埋め込み → **近似最近傍検索** → 上位 N 件を **プロンプト** に組み込んで LLM に生成させる
>
> *(... 以下、イベントベースの詳細な表、判断軸の表、推奨シナリオ表、ハイブリッド提案が続く ...)*
>
> ## 3. どちらを選ぶべきか？
>
> | 観点 | RAG + ベクトルDB が向いているケース | イベントベースが向いているケース |
> |------|--------------------------------------|-----------------------------------|
> | **目的** | *意味的に似た過去の発言やストーリー要素を汎用的に呼び出したい* | *時間軸・因果関係が重要で、直前の文脈や「今日の出来事」だけで十分* |
> | **データ量** | 数万〜数百万件の「テキスト」データを扱うときに強い | 数千〜数万件の「イベント」ならシンプルに管理できる |
> | **リアルタイム性** | 1〜2 秒以内の検索で OK なら可（インデックス更新はバッチで） | イベントは即時追加・即時取得が必要なライブ配信に最適 |
>
> ### まとめ（1行要約）
> - **RAG + ベクトルDB**は「意味的類似検索」や「大規模長期記憶」に強く、実装コストはやや高め。
> - **イベントベース**は「時系列・因果関係・リアルタイム性」に優れ、シンプルで低コスト。
> - プロジェクトのフェーズや目的に合わせて選択し、必要ならハイブリッドで相乗効果を狙うのが最適です。

**応答の特徴**:
- 非常に詳細な表形式の分析
- RAG+ベクトルDBとイベントベースの比較を5つの観点で整理（検索精度、スケーラビリティ、実装ハードル、メモリ鮮度、開発コスト）
- 典型的な構成例を具体的に記載
- 「どちらを選ぶべきか」の判断軸を6つの観点で提示
- 推奨シナリオを5つの事例で説明
- ハイブリッド運用の提案も含む
- 全体で約2000〜2500語

#### 比較まとめ

| 項目 | qwen2.5:14b | gpt-oss:120b |
|------|-------------|--------------|
| 応答時間 | 28.9秒 | 2分以上 |
| 応答の深さ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 構造化 | シンプルな箇条書き | 複数の詳細な表 |
| 実用性 | すぐに理解できる | 詳細な判断材料を提供 |
| 適した用途 | リアルタイム会話、クイックリファレンス | 技術文書作成、詳細な設計判断 |

**結論**:
- **qwen2.5:14b**は、素早く要点を掴みたい時や、ユーザーとのリアルタイムな会話に最適
- **gpt-oss:120b**は、深い分析が必要な技術判断や、詳細なドキュメント作成に最適
- 用途に応じて使い分けることで、最適なUXを実現できる

## 感想

### Good

- ✅ オープンソースで商用利用可能（Apache 2.0）
- ✅ 日本語の応答品質が高い
- ✅ 単一GPUで動作する設計
- ✅ 推論過程が見えるのは開発に便利
- ✅ qwen2.5:14bとの使い分けで最適なUXを実現できる

### 注意点

- ⚠️ モデルサイズが65GBと大きい
- ⚠️ 応答速度は小型モデルに比べて遅い
- ⚠️ 80GB GPUが推奨（CPUでも動くが実用的ではない）
- ⚠️ リアルタイム用途には向かない

## まとめ

OpenAIのgpt-oss-120bは、オープンウェイトながら高品質な推論モデルです。特に以下のようなユースケースに適しています：

- 推論過程の可視化が重要なアプリケーション
- エージェント的なタスク（関数呼び出し、ツール利用）
- 商用利用が必要なプロジェクト

Ollamaで簡単に試せるので、興味のある方はぜひ触ってみてください。

## 参考資料

- [OpenAI: Introducing gpt-oss](https://openai.com/index/introducing-gpt-oss/)
- [Ollama: gpt-oss-120b](https://ollama.com/library/gpt-oss:120b)
- [GitHub: openai/gpt-oss](https://github.com/openai/gpt-oss)
- [arXiv: gpt-oss-120b Model Card](https://arxiv.org/abs/2508.10925)

---

🤖 Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
