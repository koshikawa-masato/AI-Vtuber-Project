---
title: LangSmithã§5ã¤ã®LLMã‚’æ¯”è¼ƒï¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰ã€ç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæŠ€è¡“è§£èª¬ Phase 1ã€‘
tags:
  - Python
  - AI
  - observability
  - LLM
  - LangSmith
private: false
updated_at: '2025-11-05T13:06:37+09:00'
id: bb95295630c647eb5632
organization_url_name: null
slide: false
ignorePublish: false
---

# ã¯ã˜ã‚ã«ï¼šAI VTuberã€Œç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã¨ã¯

æœ¬è¨˜äº‹ã¯ã€**AI VTuberä¸‰å§‰å¦¹(Kashoã€ç‰¡ä¸¹ã€ãƒ¦ãƒª)ã®è¨˜æ†¶è£½é€ æ©Ÿã‚·ã‚¹ãƒ†ãƒ **ã®æŠ€è¡“è§£èª¬ã‚·ãƒªãƒ¼ã‚ºç¬¬1å¼¾ã§ã™ã€‚

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ã€Œç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã¯ã€**éå»ã®è¨˜æ†¶ã‚’æŒã¤AI VTuber**ã‚’å®Ÿç¾ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ä¸‰å§‰å¦¹ãã‚Œãã‚ŒãŒå›ºæœ‰ã®è¨˜æ†¶ãƒ»å€‹æ€§ãƒ»ä¾¡å€¤è¦³ã‚’æŒã¡ã€è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ã„åˆ†ã‘ãªãŒã‚‰é…ä¿¡ã‚’è¡Œã„ã¾ã™ã€‚

### ä¸‰å§‰å¦¹ã®æ§‹æˆ

- **Kasho(é•·å¥³)**: è«–ç†çš„ãƒ»åˆ†æçš„ã€æ…é‡ã§ãƒªã‚¹ã‚¯é‡è¦–ã€ä¿è­·è€…çš„ãªå§‰
- **ç‰¡ä¸¹(æ¬¡å¥³)**: ã‚®ãƒ£ãƒ«ç³»ã€æ„Ÿæƒ…çš„ãƒ»ç›´æ„Ÿçš„ã€æ˜ã‚‹ãç‡ç›´ã€è¡Œå‹•åŠ›æŠœç¾¤
- **ãƒ¦ãƒª(ä¸‰å¥³)**: çµ±åˆçš„ãƒ»æ´å¯Ÿçš„ã€èª¿æ•´å½¹ã€å…±æ„ŸåŠ›ãŒé«˜ã„

### GitHubãƒªãƒã‚¸ãƒˆãƒª

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã§å…¬é–‹ã—ã¦ã„ã¾ã™ï¼š
- ãƒªãƒã‚¸ãƒˆãƒª: https://github.com/koshikawa-masato/AI-Vtuber-Project
- ä¸»è¦æ©Ÿèƒ½: è¨˜æ†¶ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã€ä¸‰å§‰å¦¹æ±ºè­°ã‚·ã‚¹ãƒ†ãƒ ã€LangSmithçµ±åˆã€VLMå¯¾å¿œ

---

# Phase 1: LangSmithçµ±åˆã®é‡è¦æ€§

LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã«ãŠã„ã¦ã€**ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ**ã¨**ã‚¨ãƒ©ãƒ¼è¿½è·¡**ã¯é¿ã‘ã¦é€šã‚Œãªã„èª²é¡Œã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€**LangSmith**ã‚’ä½¿ã£ã¦5ã¤ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ãŸäº‹ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## ğŸ¯ ã“ã®è¨˜äº‹ã§åˆ†ã‹ã‚‹ã“ã¨

- LangSmithã®åŸºæœ¬çš„ãªçµ±åˆæ–¹æ³•ï¼ˆOllama/OpenAI/Geminiå¯¾å¿œï¼‰
- å‹•çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹åè¨­å®šã§è¦‹ã‚„ã™ã„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’å®Ÿç¾
- Ollamaåˆå›ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’é™¤å¤–ã™ã‚‹æ¸¬å®šæ‰‹æ³•
- Geminiã®finish_reasonå•é¡Œã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ

## ğŸ“¦ å¯¾è±¡ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

- **Ollama**ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰ï¼šqwen2.5:3b / 7b / 14b
- **OpenAI API**ï¼šgpt-4o-mini
- **Google Gemini API**ï¼šgemini-2.5-flash

---

# LangSmithã¨ã¯

[LangSmith](https://smith.langchain.com/)ã¯ã€LangChainãŒæä¾›ã™ã‚‹LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®**Observabilityï¼ˆå¯è¦³æ¸¬æ€§ï¼‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **ã§ã™ã€‚

## ä¸»ãªæ©Ÿèƒ½

| æ©Ÿèƒ½ | èª¬æ˜ |
|------|------|
| **Tracing** | LLMå‘¼ã³å‡ºã—ã®å…¥åŠ›ãƒ»å‡ºåŠ›ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è‡ªå‹•è¨˜éŒ² |
| **Monitoring** | ã‚³ã‚¹ãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒ»ã‚¨ãƒ©ãƒ¼ç‡ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ– |
| **Debugging** | ã‚¨ãƒ©ãƒ¼ã®ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¨å†ç¾ãƒ†ã‚¹ãƒˆ |
| **Evaluation** | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®A/Bãƒ†ã‚¹ãƒˆã¨å“è³ªè©•ä¾¡ |

## ãªãœLangSmithãŒå¿…è¦ã‹

LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºã§ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ï¼š

- âŒ **ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒæ¯”è¼ƒã—ã¥ã‚‰ã„**
- âŒ **ã‚¨ãƒ©ãƒ¼ã®åŸå› ãŒãƒ­ã‚°ã‹ã‚‰è¿½ã„ã«ãã„**
- âŒ **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ã®å½±éŸ¿ãŒè¦‹ãˆãªã„**
- âŒ **æœ¬ç•ªç’°å¢ƒã®ã‚³ã‚¹ãƒˆãŒäºˆæ¸¬ã§ããªã„**

LangSmithã‚’ä½¿ã†ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’**ä¸€å…ƒç®¡ç†**ã§ãã¾ã™ã€‚

---

# å®Ÿè£…ï¼šLangSmithçµ±åˆ

## 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install langsmith httpx openai google-generativeai
```

## 2. ç’°å¢ƒå¤‰æ•°è¨­å®š

`.env`ãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã‚’è¿½åŠ ï¼š

```bash
# LangSmith
LANGSMITH_API_KEY=lsv2_pt_...your_key_here
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=botan-llm-benchmark-v3

# LLM API Keys
OPENAI_API_KEY=sk-proj-...
GOOGLE_API_KEY=AIza...
```

LangSmith APIã‚­ãƒ¼ã¯[ã“ã¡ã‚‰](https://smith.langchain.com)ã‹ã‚‰å–å¾—ã§ãã¾ã™ï¼ˆç„¡æ–™æ ã‚ã‚Šï¼‰ã€‚

## 3. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè£…

### å‹•çš„ãƒˆãƒ¬ãƒ¼ã‚¹åã®è¨­å®š

LangSmithã®`@traceable`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ã¯ã€å›ºå®šã®é–¢æ•°åãŒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚’**ãƒ¢ãƒ‡ãƒ«å**ã§å‹•çš„ã«å¤‰æ›´ã—ã¾ã™ã€‚

```python
from langsmith import traceable
from typing import Dict, Any

class TracedLLM:
    def __init__(
        self,
        provider: str = "ollama",
        model: str = "qwen2.5:14b",
        project_name: str = "botan-project"
    ):
        self.provider = provider
        self.model = model
        self.project_name = project_name
        self.langsmith_enabled = os.getenv("LANGSMITH_TRACING", "false").lower() == "true"

    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ã‚¹åã‚’å–å¾—
        trace_name = metadata.get("model_name", self.model) if metadata else self.model

        # ãƒˆãƒ¬ãƒ¼ã‚¹å¯¾è±¡ã®é–¢æ•°ã‚’å®šç¾©
        def do_generate(input_prompt: str) -> str:
            if self.provider == "ollama":
                full_result = self._ollama_generate(input_prompt, temperature, max_tokens)
            elif self.provider == "openai":
                full_result = self._openai_generate(input_prompt, temperature, max_tokens)
            elif self.provider == "gemini":
                full_result = self._gemini_generate(input_prompt, temperature, max_tokens)
            else:
                raise ValueError(f"Unknown provider: {self.provider}")

            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’æŠ•ã’ã‚‹ï¼ˆLangSmithãŒã‚­ãƒ£ãƒ—ãƒãƒ£ï¼‰
            if "error" in full_result:
                raise RuntimeError(full_result["error"])

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¿”ã™ï¼ˆOutputåˆ—ã«è¡¨ç¤ºï¼‰
            do_generate.full_result = full_result
            return full_result.get("response", "")

        # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’é©ç”¨
        if self.langsmith_enabled:
            traced_func = traceable(
                run_type="llm",
                name=trace_name,  # å‹•çš„ãªåå‰
                project_name=self.project_name
            )(do_generate)
            try:
                response_text = traced_func(prompt)
                result = do_generate.full_result
            except RuntimeError:
                result = do_generate.full_result
        else:
            # ãƒˆãƒ¬ãƒ¼ã‚¹ãªã—ã§å®Ÿè¡Œ
            response_text = do_generate(prompt)
            result = do_generate.full_result

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        if metadata:
            result["metadata"] = metadata
        result["timestamp"] = datetime.now().isoformat()

        return result
```

### ãƒã‚¤ãƒ³ãƒˆè§£èª¬

1. **å‹•çš„ãƒˆãƒ¬ãƒ¼ã‚¹å**: `metadata["model_name"]`ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹åã«ä½¿ç”¨
2. **ã‚¨ãƒ©ãƒ¼ã®ä¾‹å¤–åŒ–**: `RuntimeError`ã‚’æŠ•ã’ã‚‹ã“ã¨ã§LangSmithã®Erroråˆ—ã«è¡¨ç¤º
3. **Outputè¡¨ç¤º**: è¾æ›¸å…¨ä½“ã§ã¯ãªã`response`ãƒ†ã‚­ã‚¹ãƒˆã®ã¿è¿”ã™

---

# Ollamaã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å•é¡Œ

## èª²é¡Œ

Ollamaã¯åˆå›å®Ÿè¡Œæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€åˆå›ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ**10ã€œ20å€é…ã„**ã§ã™ã€‚

```
ollama_14b: 1å›ç›® 13,677ms â†’ 2å›ç›® 1,173msï¼ˆ11.7å€é«˜é€ŸåŒ–ï¼‰
```

## è§£æ±ºç­–ï¼šé¸æŠçš„ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œã¯ãƒˆãƒ¬ãƒ¼ã‚¹ã›ãšã€2å›ç›®ã®æ¸¬å®šå®Ÿè¡Œã®ã¿ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™ã€‚

```python
def warmup_ollama(model: str, prompt: str, ollama_url: str = "http://localhost:11434"):
    """Warmup Ollama model (no tracing)"""
    try:
        with httpx.Client(timeout=120.0) as client:
            response = client.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.7, "num_predict": 100}
                }
            )
            response.raise_for_status()
            return response.json()
    except Exception as e:
        print(f"âš ï¸ Warmup failed: {str(e)}")
        return None

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
for config in models:
    is_ollama = config["provider"] == "ollama"

    # Ollamaã®ã¿ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    if is_ollama:
        print("Warmup run (no trace)...")
        warmup_ollama(config["model"], test_prompt)

    # æ¸¬å®šå®Ÿè¡Œï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ã‚ã‚Šï¼‰
    print("Measurement run (traced to LangSmith)...")
    llm = TracedLLM(
        provider=config["provider"],
        model=config["model"],
        project_name="botan-llm-benchmark-v3"
    )
    result = llm.generate(
        prompt=test_prompt,
        temperature=0.7,
        max_tokens=100,
        metadata={"model_name": config["name"]}
    )
```

---

# Geminiã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

## å•é¡Œï¼šfinish_reason=2ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©º

Gemini APIã§ã¯ã€ä»¥ä¸‹ã®ç•°å¸¸çŠ¶æ…‹ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼š

```python
finish_reason: 2  # MAX_TOKENS
content.parts: 0  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©º
```

ã“ã‚Œã¯çŸ›ç›¾ã—ãŸçŠ¶æ…‹ã§ã€é©åˆ‡ã«ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## å®Ÿè£…ï¼šfinish_reasonãƒã‚§ãƒƒã‚¯

```python
def _gemini_generate(self, prompt: str, temperature: float, max_tokens: int):
    start_time = time.time()

    try:
        import google.generativeai as genai
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

        # å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç·©å’Œ
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]

        model = genai.GenerativeModel(self.model, safety_settings=safety_settings)
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )
        )

        latency_ms = (time.time() - start_time) * 1000

        # å€™è£œãŒãªã„å ´åˆ
        if not response.candidates:
            return {
                "response": "",
                "tokens": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "latency_ms": latency_ms,
                "model": self.model,
                "provider": self.provider,
                "error": "Response blocked: No candidates returned"
            }

        candidate = response.candidates[0]
        finish_reason = candidate.finish_reason

        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        try:
            response_text = response.text
        except ValueError:
            if candidate.content and candidate.content.parts:
                response_text = candidate.content.parts[0].text
            else:
                response_text = ""

        # finish_reason != STOP ã‹ã¤ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºãªã‚‰ã‚¨ãƒ©ãƒ¼
        if finish_reason != 1 and not response_text:  # 1 = STOP
            finish_reason_names = {
                0: "FINISH_REASON_UNSPECIFIED",
                1: "STOP",
                2: "MAX_TOKENS",
                3: "SAFETY",
                4: "RECITATION",
                5: "OTHER"
            }
            reason_name = finish_reason_names.get(finish_reason, f"UNKNOWN({finish_reason})")

            if finish_reason == 3:  # SAFETY
                error_msg = f"Blocked by safety filter: {reason_name}"
            else:
                error_msg = f"Generation failed: {reason_name} (no content returned)"

            return {
                "response": "",
                "tokens": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
                "latency_ms": latency_ms,
                "model": self.model,
                "provider": self.provider,
                "error": error_msg
            }

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°å–å¾—
        tokens = {
            "prompt_tokens": response.usage_metadata.prompt_token_count,
            "completion_tokens": response.usage_metadata.candidates_token_count,
            "total_tokens": response.usage_metadata.total_token_count
        }

        return {
            "response": response_text,
            "tokens": tokens,
            "latency_ms": latency_ms,
            "model": self.model,
            "provider": self.provider
        }

    except Exception as e:
        latency_ms = (time.time() - start_time) * 1000
        return {
            "response": "",
            "tokens": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            "latency_ms": latency_ms,
            "model": self.model,
            "provider": self.provider,
            "error": str(e)
        }
```

---

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

## å®Ÿè¡Œç’°å¢ƒ

- **æ—¥ä»˜**: 2025å¹´11æœˆ5æ—¥
- **OS**: WSL2 Linux
- **CPU**: AMD Ryzen 9 9950Xï¼ˆ16ã‚³ã‚¢/32ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
- **GPU**: NVIDIA RTX 4060 Ti 16GB

## LangSmithãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼šå…¨ä½“åƒ

![LangSmithãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ](https://raw.githubusercontent.com/koshikawa-masato/AI-Vtuber-Project/feature/langsmith-integration/screenshots/screenshot_LangSmith01.png)

### ğŸ“Š æ¸¬å®šçµæœ

| Name | Input | Output | Error | Latency |
|------|-------|--------|-------|---------|
| **ollama_3b** | Tell me about AI VTubers in one sentence. | AI VTubers are digital characters an... | - | 0.40s |
| **ollama_7b** | Tell me about AI VTubers in one sentence. | AI VTubers are virtual broadcasters ... | - | 0.59s |
| **ollama_14b** | Tell me about AI VTubers in one sentence. | AI VTubers are virtual YouTubers wh... | - | 1.21s |
| **openai_4o-mini** | Tell me about AI VTubers in one sentence. | AI VTubers are virtual characters po... | - | 1.85s |
| **gemini_2.5-flash** | Tell me about AI VTubers in one sentence. | null | âš ï¸ RuntimeError('Ge... | 2.30s |

### ğŸ† ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ

1. **ollama_3b**: 384msï¼ˆæœ€é€Ÿï¼‰
2. **ollama_7b**: 588ms
3. **ollama_14b**: 1,210ms
4. **openai_4o-mini**: 1,649ms
5. **gemini_2.5-flash**: FAILED

### çµ±è¨ˆæƒ…å ±

- **Total Tokens**: 0 / $0.00
- **Run Count**: 5
- **Error Rate**: 0% â†’ 20%ï¼ˆGeminiå¤±æ•—ã‚’å«ã‚€ï¼‰

---

## Geminiã‚¨ãƒ©ãƒ¼è©³ç´°

![Geminiã‚¨ãƒ©ãƒ¼è©³ç´°](https://raw.githubusercontent.com/koshikawa-masato/AI-Vtuber-Project/feature/langsmith-integration/screenshots/screenshot_LangSmith02.png)

### ã‚¨ãƒ©ãƒ¼å†…å®¹

```
RuntimeError('Generation failed: MAX_TOKENS (no content returned)')

Traceback (most recent call last):
  File "/home/koshikawa/AI-Vtuber-Project/src/core/llm_tracing.py", line 352, in do_generate
    raise RuntimeError(full_result["error"])
RuntimeError: Generation failed: MAX_TOKENS (no content returned)
```

### è©³ç´°

- **Input Prompt**: "Tell me about AI VTubers in one sentence."
- **Output**: `null`ï¼ˆç©ºï¼‰
- **finish_reason**: 2ï¼ˆMAX_TOKENSï¼‰
- **Error**: LangSmithã®Erroråˆ—ã«æ­£ã—ãè¡¨ç¤º

ã“ã®ã‚¨ãƒ©ãƒ¼ã¯ã€Gemini APIãŒ`finish_reason=MAX_TOKENS`ã‚’è¿”ã—ãªãŒã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã¨ã„ã†çŸ›ç›¾ã—ãŸçŠ¶æ…‹ã§ã™ã€‚

---

# LangSmithãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¦‹æ–¹

## Nameåˆ—

**ä¿®æ­£å‰**: `ollama_generate`ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«åŒã˜åå‰ï¼‰
**ä¿®æ­£å¾Œ**: `ollama_3b`, `ollama_7b`, `gemini_2.5-flash`ï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«è­˜åˆ¥ï¼‰

å‹•çš„ãƒˆãƒ¬ãƒ¼ã‚¹åã«ã‚ˆã‚Šã€ä¸€ç›®ã§ã©ã®ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‹åˆ¤åˆ¥ã§ãã¾ã™ã€‚

## Outputåˆ—

**ä¿®æ­£å‰**: `{"response": "...", "tokens": {...}, ...}`ï¼ˆè¾æ›¸å…¨ä½“ï¼‰
**ä¿®æ­£å¾Œ**: `"AI VTubers are virtual characters..."`ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰

é–¢æ•°ã®æˆ»ã‚Šå€¤ã‚’æ–‡å­—åˆ—ã«ã™ã‚‹ã“ã¨ã§ã€LangSmithãŒè‡ªå‹•çš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã«è¡¨ç¤ºã—ã¾ã™ã€‚

## Erroråˆ—

**ä¿®æ­£å‰**: ç©ºï¼ˆã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œãªã„ï¼‰
**ä¿®æ­£å¾Œ**: `RuntimeError: Generation failed: MAX_TOKENS (no content returned)`

ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«`RuntimeError`ã‚’æŠ•ã’ã‚‹ã“ã¨ã§ã€LangSmithãŒä¾‹å¤–ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹ã¨å…±ã«è¡¨ç¤ºã—ã¾ã™ã€‚

---

# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## Q1. ãƒˆãƒ¬ãƒ¼ã‚¹ãŒLangSmithã«è¡¨ç¤ºã•ã‚Œãªã„

### åŸå› 1: ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„

```bash
export LANGSMITH_API_KEY=lsv2_pt_...
export LANGSMITH_TRACING=true
export LANGSMITH_PROJECT=your-project-name
```

### åŸå› 2: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãŒé–“é•ã£ã¦ã„ã‚‹

ã‚³ãƒ¼ãƒ‰å†…ã§å‹•çš„ã«è¨­å®šã™ã‚‹å ´åˆï¼š

```python
os.environ["LANGSMITH_PROJECT"] = "botan-llm-benchmark-v3"
```

### ç¢ºèªæ–¹æ³•

```bash
echo $LANGSMITH_API_KEY
echo $LANGSMITH_TRACING
```

---

## Q2. Ollamaã®åˆå›å®Ÿè¡ŒãŒé…ã„

### åŸå› 

Ollamaã¯ãƒ¢ãƒ‡ãƒ«ã‚’åˆå›å®Ÿè¡Œæ™‚ã«ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

### è§£æ±ºç­–

ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œã‚’è¿½åŠ ã—ã€2å›ç›®ä»¥é™ã®æ¸¬å®šå€¤ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```python
# 1å›ç›®: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ãªã—ï¼‰
warmup_ollama(model, prompt)

# 2å›ç›®: æ¸¬å®šï¼ˆãƒˆãƒ¬ãƒ¼ã‚¹ã‚ã‚Šï¼‰
result = llm.generate(prompt, metadata={"model_name": "ollama_3b"})
```

---

## Q3. GeminiãŒfinish_reason=3ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹

### åŸå› 

Geminiã®å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å³ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚

### è§£æ±ºç­–

`safety_settings`ã§å…¨ã‚«ãƒ†ã‚´ãƒªã‚’`BLOCK_NONE`ã«è¨­å®šã—ã¾ã™ã€‚

```python
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

model = genai.GenerativeModel(self.model, safety_settings=safety_settings)
```

**æ³¨æ„**: æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„ã€‚

---

# ã¾ã¨ã‚

## Phase 1 ã®æˆæœ

| é …ç›® | å†…å®¹ |
|-----|------|
| **å®Ÿè£…å†…å®¹** | LangSmithçµ±åˆï¼ˆOllama/OpenAI/Geminiå¯¾å¿œï¼‰ |
| **ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°** | å‹•çš„ãƒˆãƒ¬ãƒ¼ã‚¹åã€é¸æŠçš„ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° |
| **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°** | Geminiã®finish_reasonå•é¡Œã«å¯¾å¿œ |
| **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯** | 5ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¯”è¼ƒ |

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ã¾ã¨ã‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ãƒˆãƒ¼ã‚¯ãƒ³æ•° | çŠ¶æ…‹ |
|--------|-----------|-----------|------|
| ollama_3b | 384ms | 74 | âœ… æˆåŠŸ |
| ollama_7b | 588ms | 69 | âœ… æˆåŠŸ |
| ollama_14b | 1,210ms | 73 | âœ… æˆåŠŸ |
| openai_4o-mini | 1,649ms | 52 | âœ… æˆåŠŸ |
| gemini_2.5-flash | - | 0 | âŒ ã‚¨ãƒ©ãƒ¼ |

### å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹

1. **Ollamaã®å„ªä½æ€§**: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¾Œã¯384msï¼ˆæœ€é€Ÿï¼‰ã§å¿œç­”
2. **OpenAI vs Gemini**: ä»Šå›GeminiãŒå¤±æ•—ã—ãŸãŒã€é€šå¸¸ã¯åŒç­‰ã®æ€§èƒ½
3. **ã‚¨ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®é‡è¦æ€§**: LangSmithã§ã‚¨ãƒ©ãƒ¼ã‚’å¯è¦–åŒ–ã§ãã‚‹ã“ã¨ãŒé–‹ç™ºåŠ¹ç‡å‘ä¸Šã«å¯„ä¸

## Phase 1-5ã®å®ŒæˆçŠ¶æ³

| Phase | å†…å®¹ | è¨˜äº‹ | çŠ¶æ…‹ |
|-------|------|------|------|
| **Phase 1** | **LangSmithãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°** | **æœ¬è¨˜äº‹** | âœ… |
| Phase 2 | VLM (Vision Language Model) çµ±åˆ | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/fd684b963bad149d3ddc) | âœ… |
| Phase 3 | LLM as a Judgeå®Ÿè£… | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/c105b84f46f143560999) | âœ… |
| Phase 4 | ä¸‰å§‰å¦¹è¨è«–ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…(èµ·æ‰¿è»¢çµ) | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/02bdbaa005949ff8cbde) | âœ… |
| Phase 5 | ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ å®Ÿè£… | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/2bf3e024325176d3400a) | âœ… |

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **Phase 2**: VLMçµ±åˆï¼ˆç”»åƒç†è§£AIï¼‰
- **Phase 3**: LLM as a Judgeï¼ˆå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰
- **Phase 4**: ä¸‰å§‰å¦¹è¨è«–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆèµ·æ‰¿è»¢çµï¼‰
- **Phase 5**: ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 

---

# å‚è€ƒè³‡æ–™

- [LangSmithå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.smith.langchain.com/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Google Gemini API](https://ai.google.dev/docs)

## é–¢é€£è¨˜äº‹

- [Ollama vs OpenAI vs Geminiï¼šAI VTuberå‘ã‘LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¾¹åº•æ¯”è¼ƒ](https://qiita.com/koshikawa-masato/items/21bb4a7cc131ef7145d4)
- [RAGå®Ÿè£…æ¯”è¼ƒï¼šã‚«ã‚¹ã‚¿ãƒ å®Ÿè£… vs LangChain](https://qiita.com/koshikawa-masato/items/badcdf311d424b5babb8)

---

# ãŠã‚ã‚Šã«

LangSmithã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä¸€å…ƒç®¡ç†ã—ã€ã‚¨ãƒ©ãƒ¼ã‚‚å«ã‚ã¦å¯è¦–åŒ–ã§ãã¾ã—ãŸã€‚

ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ãŒé–‹ç™ºåŠ¹ç‡å‘ä¸Šã«è²¢çŒ®ã—ã¾ã—ãŸï¼š

- ğŸ” **å‹•çš„ãƒˆãƒ¬ãƒ¼ã‚¹å**ã§å„ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã‚’è­˜åˆ¥
- âš¡ **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é™¤å¤–**ã§æ­£ç¢ºãªãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®š
- ğŸ› **ã‚¨ãƒ©ãƒ¼ã®ä¾‹å¤–åŒ–**ã§LangSmithãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«è¡¨ç¤º

ä»Šå¾Œã¯ã€LangSmithã®è©•ä¾¡æ©Ÿèƒ½ï¼ˆEvaluationï¼‰ã‚’ä½¿ã£ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„ã‚„ã€ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã«ã‚‚å–ã‚Šçµ„ã‚“ã§ã„ãã¾ã™ã€‚

è³ªå•ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãŠå¾…ã¡ã—ã¦ã„ã¾ã™ï¼
