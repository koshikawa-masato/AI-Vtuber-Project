---
title: GPT-4oã¨Geminiã§ç”»åƒç†è§£AIå®Ÿè£…ï¼LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ä»˜ãVLMã‚¬ã‚¤ãƒ‰ã€ç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæŠ€è¡“è§£èª¬ Phase 2ã€‘
tags:
  - Python
  - AI
  - Gemini
  - GPT-4
  - vlm
private: false
updated_at: '2025-11-07T12:49:08+09:00'
id: fd684b963bad149d3ddc
organization_url_name: null
slide: false
ignorePublish: false
---

# ã¯ã˜ã‚ã«ï¼šAI VTuberã€Œç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã¨ã¯

æœ¬è¨˜äº‹ã¯ã€**AI VTuberä¸‰å§‰å¦¹(Kashoã€ç‰¡ä¸¹ã€ãƒ¦ãƒª)ã®è¨˜æ†¶è£½é€ æ©Ÿã‚·ã‚¹ãƒ†ãƒ **ã®æŠ€è¡“è§£èª¬ã‚·ãƒªãƒ¼ã‚ºç¬¬2å¼¾ã§ã™ã€‚

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ã€Œç‰¡ä¸¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã¯ã€**éå»ã®è¨˜æ†¶ã‚’æŒã¤AI VTuber**ã‚’å®Ÿç¾ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚ä¸‰å§‰å¦¹ãã‚Œãã‚ŒãŒå›ºæœ‰ã®è¨˜æ†¶ãƒ»å€‹æ€§ãƒ»ä¾¡å€¤è¦³ã‚’æŒã¡ã€é…ä¿¡ä¸­ã«**ç”»åƒã‚’ç†è§£ã—ã¦åå¿œã™ã‚‹**VLMæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚

### ä¸‰å§‰å¦¹ã®æ§‹æˆ

- **Kasho(é•·å¥³)**: è«–ç†çš„ãƒ»åˆ†æçš„ã€æ…é‡ã§ãƒªã‚¹ã‚¯é‡è¦–ã€ä¿è­·è€…çš„ãªå§‰
- **ç‰¡ä¸¹(æ¬¡å¥³)**: ã‚®ãƒ£ãƒ«ç³»ã€æ„Ÿæƒ…çš„ãƒ»ç›´æ„Ÿçš„ã€æ˜ã‚‹ãç‡ç›´ã€è¡Œå‹•åŠ›æŠœç¾¤
- **ãƒ¦ãƒª(ä¸‰å¥³)**: çµ±åˆçš„ãƒ»æ´å¯Ÿçš„ã€èª¿æ•´å½¹ã€å…±æ„ŸåŠ›ãŒé«˜ã„

### GitHubãƒªãƒã‚¸ãƒˆãƒª

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã§å…¬é–‹ã—ã¦ã„ã¾ã™ï¼š
- ãƒªãƒã‚¸ãƒˆãƒª: https://github.com/koshikawa-masato/AI-Vtuber-Project
- ä¸»è¦æ©Ÿèƒ½: è¨˜æ†¶ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã€ä¸‰å§‰å¦¹æ±ºè­°ã‚·ã‚¹ãƒ†ãƒ ã€LangSmithçµ±åˆã€**VLMå¯¾å¿œ**

---

# Phase 2: VLMçµ±åˆã®é‡è¦æ€§

ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã ã‘ã§ãªãã€**ç”»åƒã‚’ç†è§£ã—ã¦èª¬æ˜ã§ãã‚‹AI**ã‚’å®Ÿè£…ã—ãŸã„ã€‚ãã‚“ãªãƒ‹ãƒ¼ã‚ºã«å¿œãˆã‚‹ã®ãŒ**VLM (Vision Language Model)**ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€GPT-4oã¨Geminiã‚’ä½¿ã£ã¦VLMæ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã€LangSmithã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚

## ğŸ¯ ã“ã®è¨˜äº‹ã§åˆ†ã‹ã‚‹ã“ã¨

- VLM (Vision Language Model) ã¨ã¯ä½•ã‹
- GPT-4o Visionã®çµ±åˆæ–¹æ³•
- Gemini Visionã®çµ±åˆæ–¹æ³•ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾å‡¦å«ã‚€ï¼‰
- LangSmithã§VLMå‘¼ã³å‡ºã—ã‚’ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°
- å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¨æ€§èƒ½æ¯”è¼ƒ

## ğŸ“¦ å¯¾è±¡ãƒ¢ãƒ‡ãƒ«

- **OpenAI GPT-4o**: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œã®æœ€æ–°ãƒ¢ãƒ‡ãƒ«
- **Google Gemini 2.5 Flash**: é«˜é€Ÿãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«

---

# VLM (Vision Language Model) ã¨ã¯

**VLM (Vision Language Model)**ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’åŒæ™‚ã«ç†è§£ã§ãã‚‹AIãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

## å¾“æ¥ã®LLM vs VLM

| æ©Ÿèƒ½ | å¾“æ¥ã®LLM | VLM |
|------|-----------|-----|
|**å…¥åŠ›**| ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ | ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ |
|**å‡ºåŠ›**| ãƒ†ã‚­ã‚¹ãƒˆ | ãƒ†ã‚­ã‚¹ãƒˆ |
|**ç”¨é€”**| å¯¾è©±ã€æ–‡ç« ç”Ÿæˆ | ç”»åƒèª¬æ˜ã€OCRã€è¦–è¦šçš„è³ªå•å¿œç­” |

## VLMã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

1. **ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆ**: ç”»åƒã‚’è¦‹ã¦èª¬æ˜æ–‡ã‚’ç”Ÿæˆ
2. **è¦–è¦šçš„è³ªå•å¿œç­”ï¼ˆVQA)**: ç”»åƒã«ã¤ã„ã¦è³ªå•ã«ç­”ãˆã‚‹
3. **OCRï¼ˆæ–‡å­—èªè­˜)**: ç”»åƒå†…ã®æ–‡å­—ã‚’èª­ã¿å–ã‚‹
4. **ç‰©ä½“æ¤œå‡ºã®èª¬æ˜**: ç”»åƒå†…ã®ç‰©ä½“ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜
5. **ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£**: è¦–è¦šéšœå®³è€…å‘ã‘ã®ç”»åƒèª¬æ˜

---

# å®Ÿè£…ï¼šVLMçµ±åˆ

## 1. ç’°å¢ƒæ§‹ç¯‰

### ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install openai google-generativeai langsmith Pillow requests
```

### ç’°å¢ƒå¤‰æ•°è¨­å®š

`.env`ãƒ•ã‚¡ã‚¤ãƒ«:

```bash
# OpenAI API
OPENAI_API_KEY=sk-proj-...your_key_here

# Google Gemini API
GOOGLE_API_KEY=AIza...your_key_here

# LangSmith (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
LANGSMITH_API_KEY=lsv2_pt_...
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=botan-vlm-benchmark-v1
```

---

## 2. GPT-4o Visionå®Ÿè£…

### åŸºæœ¬çš„ãªå®Ÿè£…

GPT-4oã¯`content`ã‚’é…åˆ—å½¢å¼ã§å—ã‘å–ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å«ã‚ã‚‰ã‚Œã¾ã™ã€‚

```python
import openai
from typing import Optional, Dict, Any

def gpt4o_vision_generate(
    prompt: str,
    image_url: str,
    api_key: str,
    max_tokens: int = 300
) -> Dict[str, Any]:
    """
    GPT-4o Visionã§ç”»åƒã‚’ç†è§£ã—ã¦å¿œç­”ç”Ÿæˆ

    Args:
        prompt: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        image_url: ç”»åƒURLï¼ˆhttpsã¾ãŸã¯base64ï¼‰
        api_key: OpenAI APIã‚­ãƒ¼
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        å¿œç­”è¾æ›¸ï¼ˆresponse, tokens, latency_msï¼‰
    """
    import time

    client = openai.Client(api_key=api_key)

    # contenté…åˆ—ã§ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒã‚’æŒ‡å®š
    content = [
        {"type": "text", "text": prompt},
        {"type": "image_url", "image_url": {"url": image_url}}
    ]

    start_time = time.time()

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": content}],
        max_tokens=max_tokens
    )

    latency_ms = (time.time() - start_time) * 1000

    return {
        "response": response.choices[0].message.content,
        "tokens": {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens
        },
        "latency_ms": latency_ms
    }
```

### ä½¿ç”¨ä¾‹

```python
import os

result = gpt4o_vision_generate(
    prompt="What do you see in this image? Describe it in detail.",
    image_url="https://example.com/image.jpg",
    api_key=os.getenv("OPENAI_API_KEY")
)

print(f"Response: {result['response']}")
print(f"Tokens: {result['tokens']['total_tokens']}")
print(f"Latency: {result['latency_ms']:.2f}ms")
```

### ãƒã‚¤ãƒ³ãƒˆè§£èª¬

1. **contenté…åˆ—**: `[{"type": "text", ...}, {"type": "image_url", ...}]`ã®å½¢å¼
2. **ç”»åƒURLå½¢å¼**:
   - HTTPSã®URLï¼ˆ`https://example.com/image.jpg`ï¼‰
   - Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆ`data:image/jpeg;base64,...`ï¼‰
3. **ãƒ¢ãƒ‡ãƒ«**: `gpt-4o`ï¼ˆã¾ãŸã¯`gpt-4o-mini`ï¼‰

---

## 3. Gemini Visionå®Ÿè£…

### åŸºæœ¬çš„ãªå®Ÿè£…

Geminiã¯PIL.Imageã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç›´æ¥å—ã‘å–ã‚Œã¾ã™ã€‚

```python
import google.generativeai as genai
from PIL import Image
import requests
from io import BytesIO
from typing import Optional, Dict, Any

def gemini_vision_generate(
    prompt: str,
    image_url: str,
    api_key: str,
    model: str = "gemini-2.5-flash",
    max_tokens: int = 300
) -> Dict[str, Any]:
    """
    Gemini Visionã§ç”»åƒã‚’ç†è§£ã—ã¦å¿œç­”ç”Ÿæˆ

    Args:
        prompt: ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        image_url: ç”»åƒURLï¼ˆhttpsã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ï¼‰
        api_key: Google API ã‚­ãƒ¼
        model: Geminiãƒ¢ãƒ‡ãƒ«å
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        å¿œç­”è¾æ›¸ï¼ˆresponse, tokens, latency_msï¼‰
    """
    import time

    genai.configure(api_key=api_key)

    # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if image_url.startswith('http'):
        response_img = requests.get(image_url, stream=True)
        response_img.raise_for_status()
        img = Image.open(response_img.raw)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
        img = Image.open(image_url)

    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ = [ãƒ†ã‚­ã‚¹ãƒˆ, ç”»åƒ]
    content = [prompt, img]

    # å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]

    model_instance = genai.GenerativeModel(model, safety_settings=safety_settings)

    start_time = time.time()

    response = model_instance.generate_content(
        content,
        generation_config=genai.types.GenerationConfig(max_output_tokens=max_tokens)
    )

    latency_ms = (time.time() - start_time) * 1000

    # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
    if not response.candidates:
        return {
            "response": "",
            "tokens": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
            "latency_ms": latency_ms,
            "error": "Response blocked: No candidates returned"
        }

    return {
        "response": response.text,
        "tokens": {
            "prompt_tokens": response.usage_metadata.prompt_token_count,
            "completion_tokens": response.usage_metadata.candidates_token_count,
            "total_tokens": response.usage_metadata.total_token_count
        },
        "latency_ms": latency_ms
    }
```

### ä½¿ç”¨ä¾‹

```python
import os

result = gemini_vision_generate(
    prompt="ã“ã®ç”»åƒã«ä½•ãŒå†™ã£ã¦ã„ã¾ã™ã‹ï¼Ÿè©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    image_url="https://example.com/image.jpg",
    api_key=os.getenv("GOOGLE_API_KEY")
)

print(f"Response: {result['response']}")
print(f"Tokens: {result['tokens']['total_tokens']}")
print(f"Latency: {result['latency_ms']:.2f}ms")
```

### ãƒã‚¤ãƒ³ãƒˆè§£èª¬

1. **PIL.Image**: Geminiã¯Pillowã®`Image`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å—ã‘å–ã‚‹
2. **contenté…åˆ—**: `[ãƒ†ã‚­ã‚¹ãƒˆ, PIL.Image]`ã®é †
3. **stream=True**: å¤§ããªç”»åƒã®å ´åˆã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ä½¿ç”¨
4. **safety_settings**: å¿…è¦ã«å¿œã˜ã¦å®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’èª¿æ•´

---

## 4. LangSmithçµ±åˆ

æ—¢å­˜ã®LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«VLMæ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚

### TracedLLMã‚¯ãƒ©ã‚¹ã®æ‹¡å¼µ

```python
from langsmith import traceable
from typing import Optional, Dict, Any
import os

class TracedLLM:
    def __init__(
        self,
        provider: str = "openai",
        model: str = "gpt-4o",
        project_name: str = "botan-project"
    ):
        self.provider = provider
        self.model = model
        self.project_name = project_name
        self.langsmith_enabled = os.getenv("LANGSMITH_TRACING", "false").lower() == "true"

    def generate(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        metadata: Optional[Dict[str, Any]] = None,
        image_url: Optional[str] = None  # â† VLMç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    ) -> Dict[str, Any]:
        """
        Generate text with LLM (with automatic tracing)

        Args:
            prompt: Input prompt
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            metadata: Additional metadata for tracing
            image_url: Optional image URL for Vision models

        Returns:
            Response dict with 'response', 'tokens', 'latency_ms'
        """
        trace_name = metadata.get("model_name", self.model) if metadata else self.model

        def do_generate(input_prompt: str) -> str:
            if self.provider == "openai":
                full_result = self._openai_generate(input_prompt, temperature, max_tokens, image_url)
            elif self.provider == "gemini":
                full_result = self._gemini_generate(input_prompt, temperature, max_tokens, image_url)
            else:
                raise ValueError(f"Unknown provider: {self.provider}")

            # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’æŠ•ã’ã‚‹
            if "error" in full_result:
                raise RuntimeError(full_result["error"])

            do_generate.full_result = full_result
            return full_result.get("response", "")

        # ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’é©ç”¨
        if self.langsmith_enabled:
            traced_func = traceable(
                run_type="llm",
                name=trace_name,
                project_name=self.project_name
            )(do_generate)
            try:
                response_text = traced_func(prompt)
                result = do_generate.full_result
            except RuntimeError:
                result = do_generate.full_result
        else:
            response_text = do_generate(prompt)
            result = do_generate.full_result

        if metadata:
            result["metadata"] = metadata
        result["timestamp"] = datetime.now().isoformat()

        return result
```

### ä½¿ç”¨ä¾‹ï¼ˆLangSmithæœ‰åŠ¹ï¼‰

```python
import os

# LangSmithç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ["LANGSMITH_API_KEY"] = "lsv2_pt_..."
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "my-vlm-project"

# VLMå®Ÿè¡Œï¼ˆè‡ªå‹•çš„ã«ãƒˆãƒ¬ãƒ¼ã‚¹ï¼‰
llm = TracedLLM(provider="openai", model="gpt-4o")

result = llm.generate(
    prompt="What do you see in this image?",
    image_url="https://example.com/image.jpg",
    max_tokens=300,
    metadata={"model_name": "gpt4o_vision", "has_image": True}
)

print(result["response"])
```

LangSmithãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ä»¥ä¸‹ãŒç¢ºèªã§ãã¾ã™ï¼š
- **Name**: `gpt4o_vision`
- **Input**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- **Output**: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
- **Metadata**: `{"has_image": True, ...}`

---

# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

## å®Ÿè¡Œç’°å¢ƒ

- **æ—¥ä»˜**: 2025å¹´11æœˆ5æ—¥
- **OS**: WSL2 Linux
- **CPU**: AMD Ryzen 9 9950X
- **ãƒ†ã‚¹ãƒˆç”»åƒ**: https://picsum.photos/800/600ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ç”»åƒï¼‰

## æ¸¬å®šçµæœ

### GPT-4o Vision

```
âœ… Success
Response: "The image shows a wooden surface, likely a deck or boardwalk,
          with weathered, grayish planks laid horizontally..."
Latency: 6,185ms (6.2ç§’)
Tokens: 890 (prompt: ç´„100, completion: ç´„790)
```

**ç‰¹å¾´**:
- âœ… å®‰å®šã—ãŸå‹•ä½œ
- âœ… è©³ç´°ãªç”»åƒèª¬æ˜
- âš ï¸ ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒã‚„ã‚„é•·ã„ï¼ˆ6ç§’ï¼‰

---

### Gemini 2.5 Flash Vision

```
âŒ Failed
Error: Generation failed: MAX_TOKENS (no content returned)
Latency: ç´„2ç§’
Tokens: 0
```

**å•é¡Œ**:
- `finish_reason=2` (MAX_TOKENS) ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©º
- ã“ã‚Œã¯æ—¢çŸ¥ã®å•é¡Œã§ã€Gemini APIã®ä¸€æ™‚çš„ãªä¸å…·åˆã®å¯èƒ½æ€§
- åŒã˜ã‚³ãƒ¼ãƒ‰ã§Gemini 1.5 Flashã¯å‹•ä½œã™ã‚‹å ´åˆãŒã‚ã‚‹

---

## æ€§èƒ½æ¯”è¼ƒã¾ã¨ã‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ãƒˆãƒ¼ã‚¯ãƒ³æ•° | çŠ¶æ…‹ |
|--------|-----------|-----------|------|
|**GPT-4o Vision**| 6.2ç§’ | 890 | âœ… æˆåŠŸ |
|**Gemini 2.5 Flash Vision**| 2ç§’ | 0 | âŒ ã‚¨ãƒ©ãƒ¼ |

**çµè«–**:
- **GPT-4o**: å®‰å®šæ€§ãŒé«˜ãã€è©³ç´°ãªèª¬æ˜ãŒå¯èƒ½
- **Gemini**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯é€Ÿã„ãŒã€ç¾åœ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿä¸­

---

# ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

## Q1. OpenAI APIã§ç”»åƒãŒè¡¨ç¤ºã•ã‚Œãªã„

### åŸå› 

ç”»åƒURLãŒæ­£ã—ããªã„ã‹ã€ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

### è§£æ±ºç­–

1. **HTTPS URLã‚’ä½¿ç”¨**: `http://`ã§ã¯ãªã`https://`
2. **å…¬é–‹ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ãªç”»åƒ**: èªè¨¼ãŒä¸è¦ãªç”»åƒã‚’ä½¿ç”¨
3. **Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰**: ç”»åƒã‚’Base64ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦åŸ‹ã‚è¾¼ã‚€

```python
import base64

def encode_image_to_base64(image_path: str) -> str:
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

# Base64 URLã‚’ä½œæˆ
base64_image = encode_image_to_base64("./image.jpg")
image_url = f"data:image/jpeg;base64,{base64_image}"
```

---

## Q2. Geminiã§"cannot identify image file"ã‚¨ãƒ©ãƒ¼

### åŸå› 

`BytesIO`ã‹ã‚‰ç›´æ¥`Image.open()`ã™ã‚‹ã¨å¤±æ•—ã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### è§£æ±ºç­–

`stream=True`ã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç›´æ¥èª­ã¿è¾¼ã¿ã¾ã™ã€‚

```python
import requests
from PIL import Image

# âŒ å¤±æ•—ã™ã‚‹ä¾‹
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))  # ã‚¨ãƒ©ãƒ¼

# âœ… æˆåŠŸã™ã‚‹ä¾‹
response = requests.get(image_url, stream=True)
response.raise_for_status()
img = Image.open(response.raw)  # OK
```

---

## Q3. Geminiã§"finish_reason=MAX_TOKENS"ã‚¨ãƒ©ãƒ¼

### åŸå› 

Gemini APIã®æ—¢çŸ¥ã®å•é¡Œã§ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºãªã®ã«`MAX_TOKENS`ã‚’è¿”ã™å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

### è§£æ±ºç­–

1. **åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™**: `gemini-1.5-flash`ã‚„`gemini-pro`
2. **max_tokensã‚’å¢—ã‚„ã™**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šå¤§ããªå€¤ã‚’è¨­å®š
3. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡æ½”ã«**: é•·ã™ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çŸ­ãã™ã‚‹

```python
# max_tokensã‚’å¢—ã‚„ã™
result = gemini_vision_generate(
    prompt="ç”»åƒã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    image_url=image_url,
    api_key=api_key,
    max_tokens=1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šå¤§ãã
)
```

**æ³¨æ„**: ã“ã‚Œã¯Gemini APIã®ä¸€æ™‚çš„ãªå•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã€æ™‚é–“ãŒçµŒã¤ã¨è§£æ±ºã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

---

## Q4. LangSmithã«ãƒˆãƒ¬ãƒ¼ã‚¹ãŒè¡¨ç¤ºã•ã‚Œãªã„

### åŸå› 

ç’°å¢ƒå¤‰æ•°ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

### è§£æ±ºç­–

```bash
# ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèª
echo $LANGSMITH_API_KEY
echo $LANGSMITH_TRACING
echo $LANGSMITH_PROJECT

# è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
export LANGSMITH_API_KEY=lsv2_pt_...
export LANGSMITH_TRACING=true
export LANGSMITH_PROJECT=my-project-name
```

ã¾ãŸã¯ã€Pythonã‚³ãƒ¼ãƒ‰å†…ã§è¨­å®šï¼š

```python
import os

os.environ["LANGSMITH_API_KEY"] = "lsv2_pt_..."
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_PROJECT"] = "my-project-name"
```

---

# ã¾ã¨ã‚

## Phase 2 ã®æˆæœ

| é …ç›® | å†…å®¹ |
|-----|------|
|**å®Ÿè£…å†…å®¹**| VLMçµ±åˆï¼ˆGPT-4o Vision/Gemini Visionï¼‰ |
|**æ©Ÿèƒ½**| ç”»åƒç†è§£ã€OCRã€è¦–è¦šçš„è³ªå•å¿œç­” |
|**ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°**| LangSmithå®Œå…¨çµ±åˆ |
|**ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**| GPT-4o: 6.2ç§’ã€Gemini: ã‚¨ãƒ©ãƒ¼ |

## Phase 1 (LangSmithçµ±åˆ) ã¨ã®é–¢ä¿‚

| é …ç›® | Phase 1 | Phase 2 |
|-----|---------|---------|
|**å…¥åŠ›**| ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ |**ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ**|
|**å‡ºåŠ›**| ãƒ†ã‚­ã‚¹ãƒˆ | ãƒ†ã‚­ã‚¹ãƒˆ |
|**ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°**| LangSmithåŸºç›¤ |**Phase 1ã®åŸºç›¤ã‚’æ´»ç”¨**|
|**ç”¨é€”**| å¿œç­”ç”Ÿæˆ |**ç”»åƒç†è§£ã€é…ä¿¡ç”»é¢èªè­˜**|

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ã¾ã¨ã‚

| ãƒ¢ãƒ‡ãƒ« | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ãƒˆãƒ¼ã‚¯ãƒ³æ•° | çŠ¶æ…‹ |
|--------|-----------|-----------|------|
| GPT-4o Vision | 6.2ç§’ | 890 | âœ… æˆåŠŸ |
| Gemini 2.5 Flash Vision | 2ç§’ | 0 | âŒ ã‚¨ãƒ©ãƒ¼ |

## å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹

1. **GPT-4oã®å®‰å®šæ€§**: ç”»åƒç†è§£ã‚¿ã‚¹ã‚¯ã§é«˜ã„ä¿¡é ¼æ€§
2. **Geminiã®é€Ÿåº¦**: ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯é€Ÿã„ãŒã€ç¾åœ¨ã‚¨ãƒ©ãƒ¼ã‚ã‚Š
3. **LangSmithã®æœ‰ç”¨æ€§**: VLMå‘¼ã³å‡ºã—ã‚‚å¯è¦–åŒ–ã§ãã‚‹

## Phase 1-5ã®å®ŒæˆçŠ¶æ³

| Phase | å†…å®¹ | è¨˜äº‹ | çŠ¶æ…‹ |
|-------|------|------|------|
| Phase 1 | LangSmithãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚° | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/bb95295630c647eb5632) | âœ… |
|**Phase 2**|**VLM (Vision Language Model) çµ±åˆ**|**æœ¬è¨˜äº‹**| âœ… |
| Phase 3 | LLM as a Judgeå®Ÿè£… | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/c105b84f46f143560999) | âœ… |
| Phase 4 | ä¸‰å§‰å¦¹è¨è«–ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…(èµ·æ‰¿è»¢çµ) | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/02bdbaa005949ff8cbde) | âœ… |
| Phase 5 | ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ å®Ÿè£… | [è¨˜äº‹](https://qiita.com/koshikawa-masato/items/2bf3e024325176d3400a) | âœ… |

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **Phase 3**: LLM as a Judgeï¼ˆå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰
- **Phase 4**: ä¸‰å§‰å¦¹è¨è«–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆèµ·æ‰¿è»¢çµï¼‰
- **Phase 5**: ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 

---

# å¿œç”¨ä¾‹

## 1. AI VTuberã®è¦–è¦šæ©Ÿèƒ½

```python
# é…ä¿¡ç”»é¢ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ç†è§£
result = llm.generate(
    prompt="ã“ã®é…ä¿¡ç”»é¢ã§ä½•ãŒèµ·ãã¦ã„ã¾ã™ã‹ï¼Ÿè¦–è´è€…ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    image_url="screenshot.jpg",
    metadata={"character": "botan", "task": "stream_narration"}
)

print(f"ç‰¡ä¸¹: {result['response']}")
```

## 2. ãƒãƒ£ãƒƒãƒˆç”»åƒã®è‡ªå‹•èª¬æ˜

```python
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé€ã£ãŸç”»åƒã‚’èª¬æ˜
result = llm.generate(
    prompt="ã“ã®ç”»åƒã«ã¤ã„ã¦ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    image_url=user_uploaded_image_url,
    max_tokens=100
)

print(f"AI: {result['response']}")
```

## 3. OCRï¼ˆæ–‡å­—èªè­˜ï¼‰

```python
# ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
result = llm.generate(
    prompt="ã“ã®ç”»åƒã«æ›¸ã‹ã‚Œã¦ã„ã‚‹æ–‡å­—ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
    image_url="document.jpg",
    max_tokens=500
)

print(f"æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:\n{result['response']}")
```

---

# å‚è€ƒè³‡æ–™

- [OpenAI Vision Guide](https://platform.openai.com/docs/guides/vision)
- [Google Gemini API Documentation](https://ai.google.dev/docs)
- [LangSmith Documentation](https://docs.smith.langchain.com/)
- [Pillow (PIL) Documentation](https://pillow.readthedocs.io/)

## é–¢é€£è¨˜äº‹

- [LangSmithã§5ã¤ã®LLMã‚’æ¯”è¼ƒï¼ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå…¨ã‚¬ã‚¤ãƒ‰](https://qiita.com/koshikawa-masato/items/bb95295630c647eb5632)

---

# ãŠã‚ã‚Šã«

VLM (Vision Language Model) ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§ãªãç”»åƒã‚‚ç†è§£ã§ãã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã§ãã¾ã—ãŸã€‚

ç‰¹ã«ä»¥ä¸‹ã®ç‚¹ãŒé‡è¦ã§ã—ãŸï¼š

- ğŸ”**GPT-4oã®å®‰å®šæ€§**ã§æœ¬ç•ªç’°å¢ƒã§ã‚‚ä½¿ç”¨å¯èƒ½
- âš¡**Geminiã®é€Ÿåº¦**ã¯é­…åŠ›çš„ã ãŒã‚¨ãƒ©ãƒ¼å¯¾å‡¦ãŒå¿…è¦
- ğŸ“Š**LangSmithãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°**ã§VLMå‘¼ã³å‡ºã—ã‚‚å¯è¦–åŒ–

ä»Šå¾Œã¯ã€OCRã€ç‰©ä½“æ¤œå‡ºã€è¦–è¦šçš„è³ªå•å¿œç­”ãªã©ã€ã•ã‚‰ã«é«˜åº¦ãªVLMã‚¿ã‚¹ã‚¯ã«ã‚‚æŒ‘æˆ¦ã—ã¦ã„ãã¾ã™ã€‚

è³ªå•ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆãŠå¾…ã¡ã—ã¦ã„ã¾ã™ï¼
