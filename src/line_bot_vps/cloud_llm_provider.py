"""
ã‚¯ãƒ©ã‚¦ãƒ‰LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAI, Gemini, Claude, xAIå¯¾å¿œï¼‰

VPSç”¨: é«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆãƒ»30ç§’åˆ¶ç´„å¯¾å¿œ
"""

import os
import logging
from typing import Optional, Dict, Any, List
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import google.generativeai as genai
import anthropic
import requests

load_dotenv()

logger = logging.getLogger(__name__)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
PROMPTS_DIR = Path(__file__).parent.parent.parent / "prompts"


class CloudLLMProvider:
    """ã‚¯ãƒ©ã‚¦ãƒ‰LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAI, Gemini, Claude, xAIå¯¾å¿œï¼‰"""

    def __init__(
        self,
        provider: str = "openai",
        model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 500
    ):
        """
        åˆæœŸåŒ–

        Args:
            provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆ"openai", "gemini", "claude", "xai"ï¼‰
            model: ãƒ¢ãƒ‡ãƒ«å
            temperature: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        """
        self.provider = provider
        self.model_name = model
        self.temperature = temperature
        self.max_tokens = max_tokens

        if provider == "openai":
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment variables")

            self.client = OpenAI(api_key=api_key)
            logger.info(f"âœ… OpenAIåˆæœŸåŒ–å®Œäº†: {model}")

        elif provider == "gemini":
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found in environment variables")

            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(model)
            logger.info(f"âœ… GeminiåˆæœŸåŒ–å®Œäº†: {model}")

        elif provider == "claude":
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not found in environment variables")

            self.client = anthropic.Anthropic(api_key=api_key)
            logger.info(f"âœ… ClaudeåˆæœŸåŒ–å®Œäº†: {model}")

        elif provider == "xai":
            api_key = os.getenv("XAI_API_KEY")
            if not api_key:
                raise ValueError("XAI_API_KEY not found in environment variables")

            self.api_key = api_key
            self.client = None  # xAIã¯REST APIã®ã¿
            logger.info(f"âœ… xAIåˆæœŸåŒ–å®Œäº†: {model}")

        elif provider == "kimi":
            api_key = os.getenv("KIMI_API_KEY")
            if not api_key:
                raise ValueError("KIMI_API_KEY not found in environment variables")

            # Kimiã¯OpenAIäº’æ›APIãªã®ã§ã€OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æµç”¨
            # è©¦è¡Œ: .cn ã¨ .ai ã®ä¸¡æ–¹ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€.ai ã‚’è©¦ã™
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.moonshot.ai/v1"
            )
            logger.info(f"âœ… Kimi (Moonshot AI)åˆæœŸåŒ–å®Œäº†: {model}")

        else:
            raise ValueError(f"Unsupported provider: {provider}")

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        conversation_history: Optional[list] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

        Args:
            prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            conversation_history: ä¼šè©±å±¥æ­´ [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ­ã‚°ç”¨ï¼‰

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            if self.provider == "openai":
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})

                # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
                if conversation_history:
                    messages.extend(conversation_history)

                messages.append({"role": "user", "content": prompt})

                # OpenAI APIå‘¼ã³å‡ºã—
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )

                result = response.choices[0].message.content

            elif self.provider == "gemini":
                # Geminiã¯system_promptã¨promptã‚’çµåˆ
                full_prompt = f"{system_prompt}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼: {prompt}" if system_prompt else prompt

                # Gemini APIå‘¼ã³å‡ºã—
                response = self.client.generate_content(
                    full_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=self.temperature,
                        max_output_tokens=self.max_tokens
                    )
                )

                result = response.text

            elif self.provider == "claude":
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
                messages = []
                if conversation_history:
                    messages.extend(conversation_history)
                messages.append({"role": "user", "content": prompt})

                # ãƒ‡ãƒãƒƒã‚°: Claude APIå‘¼ã³å‡ºã—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¢ºèª
                logger.info(f"ğŸ” Claude APIå‘¼ã³å‡ºã—: model={self.model_name}, system_prompt={len(system_prompt) if system_prompt else 0}æ–‡å­—, messages={len(messages)}ä»¶")

                # Claude APIå‘¼ã³å‡ºã—
                response = self.client.messages.create(
                    model=self.model_name,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    system=system_prompt if system_prompt else "",
                    messages=messages
                )

                result = response.content[0].text

            elif self.provider == "xai":
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                if conversation_history:
                    messages.extend(conversation_history)
                messages.append({"role": "user", "content": prompt})

                # xAI APIå‘¼ã³å‡ºã—ï¼ˆREST APIï¼‰
                url = "https://api.x.ai/v1/chat/completions"
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json"
                }
                payload = {
                    "messages": messages,
                    "model": self.model_name,
                    "temperature": self.temperature,
                    "max_tokens": self.max_tokens
                }

                response = requests.post(url, json=payload, headers=headers, timeout=60)
                response.raise_for_status()
                result_json = response.json()
                result = result_json["choices"][0]["message"]["content"]

            elif self.provider == "kimi":
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆOpenAIäº’æ›ï¼‰
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})

                # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
                if conversation_history:
                    messages.extend(conversation_history)

                messages.append({"role": "user", "content": prompt})

                # Kimi APIå‘¼ã³å‡ºã—ï¼ˆOpenAIäº’æ›ï¼‰
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens
                )

                result = response.choices[0].message.content

            else:
                raise ValueError(f"Unsupported provider: {self.provider}")

            # ãƒ­ã‚°è¨˜éŒ²
            logger.info(f"âœ… LLMç”ŸæˆæˆåŠŸ ({self.provider}): {len(result)}æ–‡å­—")
            if metadata:
                logger.debug(f"   ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: {metadata}")

            return result

        except Exception as e:
            logger.error(f"âŒ LLMç”Ÿæˆã‚¨ãƒ©ãƒ¼ ({self.provider}): {e}")
            raise

    def generate_with_context(
        self,
        user_message: str,
        character_name: str,
        character_prompt: str,
        memories: Optional[str] = None,
        daily_trends: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[list] = None,
        metadata: Optional[Dict[str, Any]] = None,
        language: str = "ja"
    ) -> str:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆ

        Args:
            user_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            character_name: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å
            character_prompt: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            memories: Phase Dè¨˜æ†¶ï¼ˆä»»æ„ï¼‰
            daily_trends: ä»Šæ—¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ï¼ˆä»»æ„ï¼‰
            conversation_history: ä¼šè©±å±¥æ­´ [{"role": "user", "content": "..."}, ...]
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            language: å¿œç­”è¨€èª ("ja" or "en")

        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        system_prompt = f"""ã‚ãªãŸã¯{character_name}ã§ã™ã€‚

{character_prompt}
"""

        # Kashoã®å ´åˆã€ãŠæ‚©ã¿ç›¸è«‡ãƒ¢ãƒ¼ãƒ‰ã‚’å¼·èª¿
        if character_name == "Kasho":
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            kasho_consultation_prompt_file = PROMPTS_DIR / "kasho_consultation_system_prompt.txt"
            if kasho_consultation_prompt_file.exists():
                with open(kasho_consultation_prompt_file, 'r', encoding='utf-8') as f:
                    kasho_consultation_prompt = f.read()
                system_prompt += f"\n\n{kasho_consultation_prompt}\n"
            else:
                logger.warning(f"Kasho consultation prompt file not found: {kasho_consultation_prompt_file}")

        # è¨˜æ†¶ã‚’è¿½åŠ 
        if memories:
            system_prompt += f"\n\nã€è¨˜æ†¶ã€‘\n{memories}\n"

        # ä»Šæ—¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’è¿½åŠ 
        if daily_trends:
            def format_content(content):
                """contentã‚’æ–‡å­—åˆ—åŒ–ï¼ˆGrokå½¢å¼/RSSå½¢å¼ã«å¯¾å¿œï¼‰"""
                if isinstance(content, dict):
                    # Grokå½¢å¼: {"summary": "...", "events": [...]}
                    if 'summary' in content:
                        return content['summary'][:200]
                    # RSSå½¢å¼: {"category": "...", "items": [...]}
                    elif 'items' in content and len(content['items']) > 0:
                        first_item = content['items'][0]
                        title = first_item.get('title', '')
                        summary = first_item.get('summary', '')
                        return f"{title} - {summary}"[:200] if summary else title[:200]
                    else:
                        return str(content)[:200]
                elif isinstance(content, str):
                    return content[:200]
                else:
                    return str(content)[:200]

            trends_text = "\n".join([
                f"- {trend.get('topic', 'ãƒˆãƒ¬ãƒ³ãƒ‰')}: {format_content(trend.get('content', ''))}..."
                for trend in daily_trends
            ])

            # ãƒ‡ãƒãƒƒã‚°: ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã®å†…å®¹ã‚’ç¢ºèª
            logger.info(f"ğŸ“° ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±:\n{trends_text}")

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
            trends_prompt_file = PROMPTS_DIR / "daily_trends_system_prompt.txt"
            if trends_prompt_file.exists():
                with open(trends_prompt_file, 'r', encoding='utf-8') as f:
                    trends_prompt_template = f.read()
                system_prompt += f"\n\n{trends_prompt_template.format(trends_text=trends_text)}\n"
            else:
                logger.warning(f"Trends prompt file not found: {trends_prompt_file}")

        # è¨€èªåˆ¥æŒ‡ç¤ºã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        language_instruction_file = PROMPTS_DIR / f"language_instruction_{language}.txt"
        if language_instruction_file.exists():
            with open(language_instruction_file, 'r', encoding='utf-8') as f:
                language_instruction = f.read()
            system_prompt += f"\n\n{language_instruction}\n"
        else:
            logger.warning(f"Language instruction file not found: {language_instruction_file}")

        # ãƒ‡ãƒãƒƒã‚°: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¢ºèª
        logger.info(f"ğŸ” ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰å®Œäº†: ã‚­ãƒ£ãƒ©={character_name}, é•·ã•={len(system_prompt)}æ–‡å­—")
        logger.debug(f"ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹:\n{system_prompt[:500]}...")

        return self.generate(
            prompt=user_message,
            system_prompt=system_prompt,
            conversation_history=conversation_history,
            metadata=metadata
        )


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    # OpenAI gpt-4o-miniãƒ†ã‚¹ãƒˆ
    try:
        llm = CloudLLMProvider(provider="openai", model="gpt-4o-mini")

        response = llm.generate_with_context(
            user_message="ãŠã¯ã‚ˆã†ï¼",
            character_name="ç‰¡ä¸¹",
            character_prompt="ã‚ãªãŸã¯æ˜ã‚‹ãç¤¾äº¤çš„ãª17æ­³ã®å¥³ã®å­ã§ã™ã€‚ã‚®ãƒ£ãƒ«å£èª¿ã§è©±ã—ã¾ã™ã€‚",
            memories=None
        )

        print(f"\nå¿œç­”: {response}\n")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
