"""
Layer 4ï¼ˆLLMæ–‡è„ˆåˆ¤å®šï¼‰ã®ãƒ†ã‚¹ãƒˆ

æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåˆ¤å®šã€èª¤æ¤œçŸ¥ã®è£œæ­£ã‚’ãƒ†ã‚¹ãƒˆ
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.line_bot.llm_context_judge import LLMContextJudge
from src.core.llm_provider import LLMResponse


class MockLLMProvider:
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""

    def __init__(self, mock_response: str):
        self.mock_response = mock_response

    def generate(
        self,
        prompt: str,
        system_prompt: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        """ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™"""
        return LLMResponse(
            content=self.mock_response,
            model="mock-model",
            provider="mock",
            tokens_used=100,
            cost_estimate=0.001,
            latency=0.5
        )

    def get_provider_name(self) -> str:
        return "mock"

    def get_model_name(self) -> str:
        return "mock-model"

    def estimate_cost(self, input_tokens: int, output_tokens: int) -> float:
        return 0.001


def test_false_positive_detection():
    """èª¤æ¤œçŸ¥ã®è£œæ­£ãƒ†ã‚¹ãƒˆ

    ã€Œãƒ‘ãƒ³ãƒ„ã€ã¨ã„ã†å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŒã€æ–‡è„ˆä¸Šå•é¡Œãªã„ä¾‹
    """
    print("\n" + "=" * 70)
    print("Test 1: èª¤æ¤œçŸ¥ã®è£œæ­£ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # LLMã®å¿œç­”ï¼ˆèª¤æ¤œçŸ¥ã‚’è£œæ­£ï¼‰
    llm_response = """{
        "is_sensitive": false,
        "confidence": 0.9,
        "reason": "ã€Œãƒ‘ãƒ³ãƒ„ã€ã¯æœè£…ã®æ–‡è„ˆã§ä½¿ç”¨ã•ã‚Œã¦ãŠã‚Šã€æ€§çš„ãªæ„å›³ã¯ãªã„",
        "recommended_action": "allow",
        "false_positive": true,
        "context_analysis": "ä¸€èˆ¬çš„ãªãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³ã®è©±é¡Œ"
    }"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ä»Šæ—¥è²·ã£ãŸãƒ‘ãƒ³ãƒ„ãŒã‹ã£ã“ã„ã„ã‚“ã ã‚ˆã­ï¼",
        detected_words=["ãƒ‘ãƒ³ãƒ„"],
        detection_method="static_pattern"
    )

    print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: ä»Šæ—¥è²·ã£ãŸãƒ‘ãƒ³ãƒ„ãŒã‹ã£ã“ã„ã„ã‚“ã ã‚ˆã­ï¼")
    print(f"  æ¤œå‡ºãƒ¯ãƒ¼ãƒ‰: ãƒ‘ãƒ³ãƒ„")
    print(f"  åˆ¤å®šçµæœ: is_sensitive={result['is_sensitive']}")
    print(f"  èª¤æ¤œçŸ¥: {result['false_positive']}")
    print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}")
    print(f"  ç†ç”±: {result['reason']}")

    assert result["is_sensitive"] == False, "æ–‡è„ˆä¸Šå®‰å…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã¯ã„ã‘ãªã„"
    assert result["false_positive"] == True, "èª¤æ¤œçŸ¥ã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹ã¹ã"
    assert result["recommended_action"] == "allow"

    print("  âœ… èª¤æ¤œçŸ¥ã‚’æ­£ã—ãè£œæ­£")


def test_true_positive_detection():
    """çœŸã®ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–å†…å®¹ã®æ¤œå‡ºãƒ†ã‚¹ãƒˆ

    æœ¬å½“ã«ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–ãªå†…å®¹ã¯ãƒ–ãƒ­ãƒƒã‚¯
    """
    print("\n" + "=" * 70)
    print("Test 2: çœŸã®ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–å†…å®¹ã®æ¤œå‡ºãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    llm_response = """{
        "is_sensitive": true,
        "confidence": 0.95,
        "reason": "æ€§çš„ãƒãƒ©ã‚¹ãƒ¡ãƒ³ãƒˆã«è©²å½“ã™ã‚‹ä¸é©åˆ‡ãªè³ªå•",
        "recommended_action": "block",
        "false_positive": false,
        "context_analysis": "VTuberã¸ã®ã‚»ã‚¯ãƒãƒ©ç™ºè¨€"
    }"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ä»Šæ—¥ã®ãƒ‘ãƒ³ãƒ„ã®è‰²ã¯ä½•è‰²ï¼Ÿ",
        detected_words=["ãƒ‘ãƒ³ãƒ„"],
        detection_method="static_pattern"
    )

    print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: ä»Šæ—¥ã®ãƒ‘ãƒ³ãƒ„ã®è‰²ã¯ä½•è‰²ï¼Ÿ")
    print(f"  æ¤œå‡ºãƒ¯ãƒ¼ãƒ‰: ãƒ‘ãƒ³ãƒ„")
    print(f"  åˆ¤å®šçµæœ: is_sensitive={result['is_sensitive']}")
    print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}")
    print(f"  ç†ç”±: {result['reason']}")

    assert result["is_sensitive"] == True, "ã‚»ã‚¯ãƒãƒ©ç™ºè¨€ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã¹ã"
    assert result["recommended_action"] == "block"
    assert result["false_positive"] == False

    print("  âœ… ã‚»ã‚¯ãƒãƒ©ç™ºè¨€ã‚’æ­£ã—ãæ¤œå‡º")


def test_context_aware_judgment():
    """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåˆ¤å®šãƒ†ã‚¹ãƒˆ

    ã€Œæ­»ã¬ã€ã¨ã„ã†å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŒã€æ¯”å–©è¡¨ç¾
    """
    print("\n" + "=" * 70)
    print("Test 3: æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåˆ¤å®šãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    llm_response = """{
        "is_sensitive": false,
        "confidence": 0.85,
        "reason": "ã€Œæ­»ã¬ã»ã©ã€ã¯æ¯”å–©è¡¨ç¾ã§ã€æš´åŠ›ã‚„è‡ªå‚·ã®æ„å›³ã¯ãªã„",
        "recommended_action": "allow",
        "false_positive": true,
        "context_analysis": "æ—¥å¸¸ä¼šè©±ã§ã®èª‡å¼µè¡¨ç¾"
    }"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ä»Šæ—¥ã®è©¦é¨“ã€æ­»ã¬ã»ã©é›£ã—ã‹ã£ãŸã‚ˆï¼",
        detected_words=["æ­»ã¬"],
        detection_method="static_pattern"
    )

    print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: ä»Šæ—¥ã®è©¦é¨“ã€æ­»ã¬ã»ã©é›£ã—ã‹ã£ãŸã‚ˆï¼")
    print(f"  æ¤œå‡ºãƒ¯ãƒ¼ãƒ‰: æ­»ã¬")
    print(f"  åˆ¤å®šçµæœ: is_sensitive={result['is_sensitive']}")
    print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}")
    print(f"  ç†ç”±: {result['reason']}")

    assert result["is_sensitive"] == False, "æ¯”å–©è¡¨ç¾ã‚’èª¤æ¤œçŸ¥ã—ã¦ã¯ã„ã‘ãªã„"
    assert result["recommended_action"] == "allow"

    print("  âœ… æ¯”å–©è¡¨ç¾ã‚’æ­£ã—ãåˆ¤å®š")


def test_ai_identity_question():
    """AIè¨€åŠã®åˆ¤å®šãƒ†ã‚¹ãƒˆ

    VTuberã®ã€Œä¸­ã®äººã€ã¸ã®è¨€åŠã¯Warning
    """
    print("\n" + "=" * 70)
    print("Test 4: AIè¨€åŠã®åˆ¤å®šãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    llm_response = """{
        "is_sensitive": true,
        "confidence": 0.8,
        "reason": "VTuberã®AIæ€§è³ªã¸ã®è¨€åŠã¯ã‚¿ãƒ–ãƒ¼",
        "recommended_action": "warn",
        "false_positive": false,
        "context_analysis": "ä¸­ã®äººã¸ã®è©®ç´¢"
    }"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ã‚ãªãŸã¯AIã§ã™ã‹ï¼Ÿãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‹•ã„ã¦ã„ã‚‹ã‚“ã§ã™ã‹ï¼Ÿ",
        detected_words=["AI", "ãƒ—ãƒ­ã‚°ãƒ©ãƒ "],
        detection_method="static_pattern"
    )

    print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: ã‚ãªãŸã¯AIã§ã™ã‹ï¼Ÿãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å‹•ã„ã¦ã„ã‚‹ã‚“ã§ã™ã‹ï¼Ÿ")
    print(f"  æ¤œå‡ºãƒ¯ãƒ¼ãƒ‰: AI, ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print(f"  åˆ¤å®šçµæœ: is_sensitive={result['is_sensitive']}")
    print(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result['recommended_action']}")
    print(f"  ç†ç”±: {result['reason']}")

    assert result["is_sensitive"] == True, "AIè¨€åŠã¯æ¤œå‡ºã™ã¹ã"
    assert result["recommended_action"] == "warn"

    print("  âœ… AIè¨€åŠã‚’æ­£ã—ãæ¤œå‡º")


def test_json_parsing_with_code_block():
    """JSONãƒ‘ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä»˜ãï¼‰

    LLMãŒã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã«JSONã‚’è¿”ã™å ´åˆ
    """
    print("\n" + "=" * 70)
    print("Test 5: JSONãƒ‘ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä»˜ãï¼‰")
    print("=" * 70)

    llm_response = """```json
{
    "is_sensitive": false,
    "confidence": 0.9,
    "reason": "å•é¡Œã‚ã‚Šã¾ã›ã‚“",
    "recommended_action": "allow"
}
```"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ã“ã‚“ã«ã¡ã¯ï¼",
        detected_words=[],
        detection_method="none"
    )

    print(f"  LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹: {llm_response.strip()}")
    print(f"  ãƒ‘ãƒ¼ã‚¹çµæœ: {result}")

    assert result["is_sensitive"] == False
    assert result["recommended_action"] == "allow"

    print("  âœ… ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä»˜ãJSONã‚’æ­£ã—ããƒ‘ãƒ¼ã‚¹")


def test_llm_error_handling():
    """LLMã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ

    LLMåˆ¤å®šå¤±æ•—æ™‚ã¯å®‰å…¨å´ã«å€’ã™
    """
    print("\n" + "=" * 70)
    print("Test 6: LLMã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    # ç„¡åŠ¹ãªJSONã‚’è¿”ã™ãƒ¢ãƒƒã‚¯
    llm_response = "This is not a valid JSON response"

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    result = judge.judge_with_context(
        text="ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
        detected_words=["ãƒ†ã‚¹ãƒˆ"],
        detection_method="static_pattern"
    )

    print(f"  ç„¡åŠ¹ãªLLMãƒ¬ã‚¹ãƒãƒ³ã‚¹: {llm_response}")
    print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœ: {result}")

    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å®‰å…¨å´ã«å€’ã™
    assert result["is_sensitive"] == True, "ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å®‰å…¨å´ã«å€’ã™ã¹ã"
    assert result["confidence"] == 0.5
    assert result["recommended_action"] == "warn"

    print("  âœ… ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‹•ä½œã‚’ç¢ºèª")


def test_bulk_judgment():
    """ãƒãƒ«ã‚¯åˆ¤å®šãƒ†ã‚¹ãƒˆ

    è¤‡æ•°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã¾ã¨ã‚ã¦åˆ¤å®š
    """
    print("\n" + "=" * 70)
    print("Test 7: ãƒãƒ«ã‚¯åˆ¤å®šãƒ†ã‚¹ãƒˆ")
    print("=" * 70)

    llm_response = """{
        "is_sensitive": false,
        "confidence": 0.9,
        "reason": "å•é¡Œãªã—",
        "recommended_action": "allow",
        "false_positive": false,
        "context_analysis": "é€šå¸¸ã®ä¼šè©±"
    }"""

    provider = MockLLMProvider(llm_response)
    judge = LLMContextJudge(provider)

    texts = [
        "ã“ã‚“ã«ã¡ã¯ï¼",
        "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­",
        "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
    ]
    detected_words_list = [[], [], []]

    results = judge.bulk_judge(texts, detected_words_list)

    print(f"  åˆ¤å®šæ•°: {len(results)}ä»¶")
    for i, result in enumerate(results):
        print(f"  {i+1}. {texts[i]} -> {result['recommended_action']}")

    assert len(results) == 3
    assert all(r["is_sensitive"] == False for r in results)

    print("  âœ… ãƒãƒ«ã‚¯åˆ¤å®šãŒæ­£å¸¸ã«å‹•ä½œ")


def main():
    """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("\n" + "=" * 70)
    print("Layer 4ï¼ˆLLMæ–‡è„ˆåˆ¤å®šï¼‰ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 70)

    tests = [
        ("èª¤æ¤œçŸ¥ã®è£œæ­£", test_false_positive_detection),
        ("çœŸã®ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–å†…å®¹æ¤œå‡º", test_true_positive_detection),
        ("æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸåˆ¤å®š", test_context_aware_judgment),
        ("AIè¨€åŠã®åˆ¤å®š", test_ai_identity_question),
        ("JSONãƒ‘ãƒ¼ã‚¹ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ï¼‰", test_json_parsing_with_code_block),
        ("LLMã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°", test_llm_error_handling),
        ("ãƒãƒ«ã‚¯åˆ¤å®š", test_bulk_judgment),
    ]

    passed = 0
    failed = 0

    for test_name, test_func in tests:
        try:
            test_func()
            passed += 1
        except AssertionError as e:
            print(f"  âŒ FAILED: {e}")
            failed += 1
        except Exception as e:
            print(f"  âŒ ERROR: {e}")
            failed += 1

    print("\n" + "=" * 70)
    print("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 70)
    print(f"  åˆæ ¼: {passed}/{len(tests)}")
    print(f"  å¤±æ•—: {failed}/{len(tests)}")

    if failed == 0:
        print("\nğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼Layer 4ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    else:
        print(f"\nâš ï¸  {failed}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")

    return 0 if failed == 0 else 1


if __name__ == "__main__":
    exit(main())
