---
title: Ollama vs OpenAI vs Geminiï¼šAI VTuberå‘ã‘LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¾¹åº•æ¯”è¼ƒã€é€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆãƒ»ä¿¡é ¼æ€§ã€‘
tags:
  - Python
  - AI
  - LLM
  - Ollama
  - Gemini
private: false
updated_at: '2025-11-04'
id: null
organization_url_name: null
slide: false
ignorePublish: false
---

# ã¯ã˜ã‚ã«

AI VTuberãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã‚’é–‹ç™ºã™ã‚‹éš›ã€**LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®é¸æŠ**ã¯æ¥µã‚ã¦é‡è¦ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€ä»¥ä¸‹3ã¤ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å®Ÿéš›ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã¦æ¯”è¼ƒã—ã¾ã—ãŸï¼š

- **Ollama**ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰ï¼šqwen2.5:3b / 7b / 14b
- **OpenAI API**ï¼šgpt-4o-mini
- **Google Gemini API**ï¼šgemini-2.5-flash

## ğŸ¯ ã“ã®è¨˜äº‹ã§åˆ†ã‹ã‚‹ã“ã¨

- ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰ã¨ã‚¯ãƒ©ã‚¦ãƒ‰LLMï¼ˆOpenAI/Geminiï¼‰ã®æ€§èƒ½æ¯”è¼ƒ
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹é€Ÿåº¦ãƒ»ã‚³ã‚¹ãƒˆãƒ»ä¿¡é ¼æ€§ã®å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿
- AI VTuberå‘ã‘ã®æœ€é©ãªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- æœˆé–“ã‚³ã‚¹ãƒˆ99%å‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•

---

# ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼

## ãƒ†ã‚¹ãƒˆç’°å¢ƒ

- **æ—¥ä»˜**: 2025å¹´11æœˆ4æ—¥
- **OS**: WSL2 Linux
- **CPU**: AMD Ryzen 9 9950Xï¼ˆ16ã‚³ã‚¢/32ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
- **RAM**: 128GB DDR5-5600
- **GPU**: NVIDIA RTX 4060 Ti 16GB

## ãƒ†ã‚¹ãƒˆæ‰‹æ³•

å„ãƒ¢ãƒ‡ãƒ«ã§**4å›**ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼š
- **1å›ç›®**: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆçµæœã‹ã‚‰é™¤å¤–ï¼‰
- **2ã€œ4å›ç›®**: æ¸¬å®šå¯¾è±¡ï¼ˆå¹³å‡ãƒ»æ¨™æº–åå·®ãƒ»æœ€å°/æœ€å¤§ã‚’è¨ˆç®—ï¼‰

ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼š
```python
# Ollamaï¼ˆæ—¥æœ¬èªï¼‰
prompt = "ã“ã‚“ã«ã¡ã¯"
system = "ã‚ãªãŸã¯ç‰¡ä¸¹ã§ã™ã€‚ã‚®ãƒ£ãƒ«å£èª¿ã§è©±ã—ã¾ã™ã€‚"

# OpenAI/Geminiï¼ˆè‹±èª - ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å›é¿ã®ãŸã‚ï¼‰
prompt = "Hello"
system = "You are a helpful AI assistant."
```

## ğŸ“ˆ çµæœä¸€è¦§

| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | ãƒ¢ãƒ‡ãƒ« | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | æ¨™æº–åå·® | æœ€å°/æœ€å¤§ | ã‚³ã‚¹ãƒˆ/å› | æˆåŠŸç‡ |
|------------|--------|--------------|---------|-----------|----------|--------|
| **Ollama (ãƒ­ãƒ¼ã‚«ãƒ«)** | qwen2.5:3b | **0.368ç§’** | 0.079ç§’ | 0.291ã€œ0.450ç§’ | **$0.00** | **100%** |
| Ollama (ãƒ­ãƒ¼ã‚«ãƒ«) | qwen2.5:7b | 0.817ç§’ | 0.460ç§’ | 0.409ã€œ1.317ç§’ | $0.00 | 100% |
| Ollama (ãƒ­ãƒ¼ã‚«ãƒ«) | qwen2.5:14b | 1.499ç§’ | 0.486ç§’ | 0.990ã€œ1.958ç§’ | $0.00 | 100% |
| OpenAI (ã‚¯ãƒ©ã‚¦ãƒ‰) | gpt-4o-mini | 1.717ç§’ | 0.247ç§’ | 1.570ã€œ2.002ç§’ | $0.000015 | 100% |
| Gemini (ã‚¯ãƒ©ã‚¦ãƒ‰) | gemini-2.5-flash | 1.692ç§’ | 0.679ç§’ | 1.212ã€œ2.172ç§’ | $0.000003 | **75%** |

---

# ğŸ” è©³ç´°åˆ†æ

## 1. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆå¿œç­”é€Ÿåº¦ï¼‰æ¯”è¼ƒ

### ğŸ¥‡ æœ€é€Ÿï¼šOllama qwen2.5:3bï¼ˆ0.368ç§’ï¼‰

- **OpenAIã®4.7å€é«˜é€Ÿ**
- **æœ€ã‚‚å®‰å®š**ï¼ˆæ¨™æº–åå·®: 0.079ç§’ï¼‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡ã«æœ€é©

### ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®å½±éŸ¿ï¼ˆOllamaï¼‰

åˆå›å®Ÿè¡Œæ™‚ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼š

| ãƒ¢ãƒ‡ãƒ« | åˆå›ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰ | 2å›ç›®ä»¥é™ï¼ˆå¹³å‡ï¼‰ | ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ— |
|--------|---------------------|----------------|-------------|
| qwen2.5:7b | 8.345ç§’ | 0.817ç§’ | **10å€** |
| qwen2.5:14b | 9.150ç§’ | 1.499ç§’ | **6å€** |

**é‡è¦**ï¼šæœ¬ç•ªç’°å¢ƒã§ã¯ãƒ¢ãƒ‡ãƒ«ã‚’å¸¸é§ã•ã›ã‚‹ã“ã¨ã§ã€ã“ã®é…å»¶ã‚’å›é¿ã§ãã¾ã™ã€‚

## 2. ã‚³ã‚¹ãƒˆæ¯”è¼ƒ

### ğŸ† ã‚³ã‚¹ãƒˆå‹è€…ï¼šOllamaï¼ˆå®Œå…¨ç„¡æ–™ï¼‰

æœˆé–“1ä¸‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆï¼š

| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | ã‚³ã‚¹ãƒˆ/å› | æœˆé–“ã‚³ã‚¹ãƒˆï¼ˆ1ä¸‡å›ï¼‰ | Ollamaã¨ã®å·®é¡ |
|------------|----------|------------------|--------------|
| **Ollama** | $0.00 | **$0.00** | - |
| Gemini | $0.000003 | $0.03 | +$0.03 |
| OpenAI | $0.000015 | $15.00 | +$15.00 |

ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–“ã§ã¯ï¼š
- **Gemini**ãŒOpenAIã‚ˆã‚Š**80%å®‰ã„**ï¼ˆ$0.000003 vs $0.000015ï¼‰

## 3. ä¿¡é ¼æ€§æ¯”è¼ƒ

| ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ | æˆåŠŸç‡ | å¤±æ•—ç†ç”± |
|------------|--------|---------|
| Ollama | **100%** (12/12) | ãªã— |
| OpenAI | **100%** (4/4) | ãªã— |
| Gemini | **75%** (3/4) | ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆæ—¥æœ¬èªã®ã€Œã‚®ãƒ£ãƒ«å£èª¿ã€ã§ãƒ–ãƒ­ãƒƒã‚¯ï¼‰ |

### Geminiã®æ³¨æ„ç‚¹

æ—¥æœ¬èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒç™ºå‹•ã—ã‚„ã™ã„ï¼š

```python
# âŒ ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸä¾‹
finish_reason = 2  # SAFETY (ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
prompt = "ã‚ãªãŸã¯ç‰¡ä¸¹ã§ã™ã€‚ã‚®ãƒ£ãƒ«å£èª¿ã§è©±ã—ã¾ã™ã€‚"

# âœ… æˆåŠŸã—ãŸä¾‹
prompt = "You are a helpful AI assistant."
```

**å›é¿ç­–**ï¼š
- è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
- ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®šã‚’èª¿æ•´ï¼ˆ`safety_settings`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

---

# ğŸ—ï¸ æ¨å¥¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼šãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ

## 3æ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

```
å„ªå…ˆåº¦1: Ollama qwen2.5:3bï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
   â†“ ã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¦ãƒ³æ™‚
å„ªå…ˆåº¦2: Gemini 2.5 Flashï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
   â†“ å“è³ªé‡è¦–æ™‚ã®ã¿
å„ªå…ˆåº¦3: OpenAI gpt-4o-miniï¼ˆç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼‰
```

### ãƒ¡ãƒªãƒƒãƒˆ

1. **95%ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’Ollamaã§å‡¦ç†**ï¼ˆç„¡æ–™ï¼‰
2. **Geminiã§ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**ï¼ˆè¶…ä½ã‚³ã‚¹ãƒˆ: $0.000003/å›ï¼‰
3. **OpenAIã¯å“è³ªãŒæœ€é‡è¦ãªå ´åˆã®ã¿**

### ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ

æœˆé–“1ä¸‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å ´åˆï¼š

| æ§‹æˆ | æœˆé–“ã‚³ã‚¹ãƒˆ | å‰Šæ¸›ç‡ |
|------|----------|--------|
| OpenAIå˜ç‹¬ | $150.00 | - |
| ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ï¼ˆ95% Ollama + 5% Geminiï¼‰ | **$0.15** | **99.9%å‰Šæ¸›** |

---

# ğŸ’» å®Ÿè£…ä¾‹

## 1. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æŠ½è±¡åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Optional

@dataclass
class LLMResponse:
    """çµ±ä¸€ã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼"""
    content: str
    model: str
    provider: str
    tokens_used: Optional[int]
    cost_estimate: float
    latency: float

class BaseLLMProvider(ABC):
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        pass

    @abstractmethod
    def get_provider_name(self) -> str:
        pass

    @abstractmethod
    def get_model_name(self) -> str:
        pass
```

## 2. Ollamaå®Ÿè£…

```python
import requests
import time
from typing import Optional

class OllamaProvider(BaseLLMProvider):
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "qwen2.5:3b"
    ):
        self.base_url = base_url
        self.model = model

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        start_time = time.time()

        payload = {
            "model": self.model,
            "prompt": prompt,
            "temperature": temperature,
            "num_predict": max_tokens,
            "stream": False
        }

        if system_prompt:
            payload["system"] = system_prompt

        response = requests.post(
            f"{self.base_url}/api/generate",
            json=payload,
            timeout=300
        )
        response.raise_for_status()

        latency = time.time() - start_time
        data = response.json()

        return LLMResponse(
            content=data.get("response", ""),
            model=self.model,
            provider="ollama",
            tokens_used=None,
            cost_estimate=0.0,  # ãƒ­ãƒ¼ã‚«ãƒ«ãªã®ã§ç„¡æ–™
            latency=latency
        )

    def get_provider_name(self) -> str:
        return "ollama"

    def get_model_name(self) -> str:
        return self.model
```

## 3. OpenAIå®Ÿè£…

```python
import os
from openai import OpenAI

class OpenAIProvider(BaseLLMProvider):
    PRICING = {
        "gpt-4o-mini": {"input": 0.150, "output": 0.600},  # $/1M tokens
    }

    def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-mini"):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key not provided")

        self.model = model
        self.client = OpenAI(api_key=self.api_key)

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        start_time = time.time()

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )

        latency = time.time() - start_time
        usage = response.usage

        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        pricing = self.PRICING.get(self.model, self.PRICING["gpt-4o-mini"])
        cost = (
            (usage.prompt_tokens / 1_000_000) * pricing["input"] +
            (usage.completion_tokens / 1_000_000) * pricing["output"]
        )

        return LLMResponse(
            content=response.choices[0].message.content,
            model=self.model,
            provider="openai",
            tokens_used=usage.total_tokens,
            cost_estimate=cost,
            latency=latency
        )

    def get_provider_name(self) -> str:
        return "openai"

    def get_model_name(self) -> str:
        return self.model
```

## 4. Geminiå®Ÿè£…

```python
import google.generativeai as genai

class GeminiProvider(BaseLLMProvider):
    PRICING = {
        "gemini-2.5-flash": {"input": 0.075, "output": 0.30},  # $/1M tokens
    }

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gemini-2.5-flash"
    ):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("Google API key not provided")

        self.model = model
        genai.configure(api_key=self.api_key)
        self.client = genai.GenerativeModel(model)

    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> LLMResponse:
        start_time = time.time()

        # Geminiã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

        response = self.client.generate_content(
            full_prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )
        )

        latency = time.time() - start_time

        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³â‰ˆ4æ–‡å­—ï¼‰
        input_tokens = len(full_prompt) // 4
        output_tokens = len(response.text) // 4

        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        pricing = self.PRICING.get(self.model, self.PRICING["gemini-2.5-flash"])
        cost = (
            (input_tokens / 1_000_000) * pricing["input"] +
            (output_tokens / 1_000_000) * pricing["output"]
        )

        return LLMResponse(
            content=response.text,
            model=self.model,
            provider="gemini",
            tokens_used=input_tokens + output_tokens,
            cost_estimate=cost,
            latency=latency
        )

    def get_provider_name(self) -> str:
        return "gemini"

    def get_model_name(self) -> str:
        return self.model
```

## 5. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆã®å®Ÿè£…

```python
class HybridLLMProvider:
    """å„ªå…ˆé †ä½ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ"""

    def __init__(self):
        self.providers = [
            OllamaProvider(model="qwen2.5:3b"),  # å„ªå…ˆåº¦1
            GeminiProvider(model="gemini-2.5-flash"),  # å„ªå…ˆåº¦2
            OpenAIProvider(model="gpt-4o-mini"),  # å„ªå…ˆåº¦3
        ]

    def generate(self, prompt: str, **kwargs) -> LLMResponse:
        for provider in self.providers:
            try:
                return provider.generate(prompt, **kwargs)
            except Exception as e:
                print(f"Provider {provider.get_provider_name()} failed: {e}")
                continue

        raise RuntimeError("All providers failed")

# ä½¿ç”¨ä¾‹
hybrid = HybridLLMProvider()
response = hybrid.generate(
    prompt="ã“ã‚“ã«ã¡ã¯",
    system_prompt="ã‚ãªãŸã¯ç‰¡ä¸¹ã§ã™ã€‚ã‚®ãƒ£ãƒ«å£èª¿ã§è©±ã—ã¾ã™ã€‚"
)

print(f"Provider: {response.provider}")
print(f"Latency: {response.latency:.3f}s")
print(f"Cost: ${response.cost_estimate:.6f}")
print(f"Response: {response.content}")
```

---

# ğŸ¬ AI VTuberå‘ã‘ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥æ¨å¥¨

## ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡

- **æ¨å¥¨**: Ollama qwen2.5:3b
- **ç†ç”±**: æœ€é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆ0.368ç§’ï¼‰ã€ç„¡æ–™
- **è¦ä»¶**: RTX 4060 Ti 16GBä»¥ä¸Šã®GPU

## é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆ

- **æ¨å¥¨**: Ollama qwen2.5:7b
- **ç†ç”±**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå“è³ªã¨é€Ÿåº¦ã€ç„¡æ–™

## ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ¤œè¨¼

- **æ¨å¥¨**: Gemini 2.5 Flash
- **ç†ç”±**: ã‚¯ãƒ©ã‚¦ãƒ‰ã§æ‰‹è»½ã€è¶…ä½ã‚³ã‚¹ãƒˆï¼ˆ$0.000003/å›ï¼‰

## æœ¬ç•ªç’°å¢ƒï¼ˆå“è³ªé‡è¦–ï¼‰

- **æ¨å¥¨**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ
- **æ§‹æˆ**: 95% Ollama + 5% Gemini/OpenAI
- **ãƒ¡ãƒªãƒƒãƒˆ**: é«˜é€Ÿãƒ»ä½ã‚³ã‚¹ãƒˆãƒ»é«˜å¯ç”¨æ€§

---

# ğŸ“Œ é‡è¦ãªç™ºè¦‹ã¨æ³¨æ„ç‚¹

## Ollamaã®æ³¨æ„ç‚¹

### 1. ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ™‚é–“

åˆå›å®Ÿè¡Œæ™‚ã«8ã€œ9ç§’ã‹ã‹ã‚Šã¾ã™ï¼š

```bash
# åˆå›ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼‰
$ time ollama run qwen2.5:7b "Hello"
# 8.345ç§’

# 2å›ç›®ä»¥é™
$ time ollama run qwen2.5:7b "Hello"
# 0.817ç§’ (10å€é«˜é€Ÿï¼)
```

**å¯¾ç­–**: systemdã‚„Dockerã§Ollamaã‚µãƒ¼ãƒ“ã‚¹ã‚’å¸¸é§ã•ã›ã‚‹

### 2. GPUè¦ä»¶

| ãƒ¢ãƒ‡ãƒ« | æœ€ä½GPU VRAM | æ¨å¥¨GPU VRAM |
|--------|-------------|-------------|
| qwen2.5:3b | 4GB | 8GB |
| qwen2.5:7b | 8GB | 12GB |
| qwen2.5:14b | 12GB | 16GB |

## Geminiã®æ³¨æ„ç‚¹

### ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼

æ—¥æœ¬èªã®ç‰¹å®šãƒ•ãƒ¬ãƒ¼ã‚ºã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ï¼š

```python
# âŒ ãƒ–ãƒ­ãƒƒã‚¯ä¾‹
response.finish_reason = 2  # SAFETY
# "ã‚®ãƒ£ãƒ«å£èª¿"ãŒã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«å¼•ã£ã‹ã‹ã£ãŸ

# âœ… å›é¿ç­–1: è‹±èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
prompt = "You are a cheerful AI assistant"

# âœ… å›é¿ç­–2: ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®šã‚’èª¿æ•´
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
}
```

---

# ğŸ”¬ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œæ–¹æ³•

## ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. Ollamaã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -fsSL https://ollama.com/install.sh | sh

# 2. ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
ollama pull qwen2.5:3b
ollama pull qwen2.5:7b
ollama pull qwen2.5:14b

# 3. Pythonç’°å¢ƒæ§‹ç¯‰
python3 -m venv venv
source venv/bin/activate
pip install openai google-generativeai python-dotenv

# 4. APIã‚­ãƒ¼è¨­å®šï¼ˆ.envï¼‰
echo "OPENAI_API_KEY=your_openai_key" >> .env
echo "GOOGLE_API_KEY=your_google_key" >> .env
```

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

```python
#!/usr/bin/env python3
"""LLM Provider Benchmark with Warm-up"""

import statistics
from providers import OllamaProvider, OpenAIProvider, GeminiProvider

ITERATIONS = 4  # 1å›ç›®=ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã€2ã€œ4å›ç›®=æ¸¬å®š

def benchmark_provider(provider, name):
    print(f"\n{'='*70}")
    print(f"Testing: {name}")
    print(f"{'='*70}")

    latencies = []

    for i in range(ITERATIONS):
        is_warmup = (i == 0)
        label = "[WARM-UP]" if is_warmup else "[MEASURE]"

        response = provider.generate(
            prompt="Hello",
            system_prompt="You are a helpful AI assistant.",
            temperature=0.7,
            max_tokens=100
        )

        latencies.append(response.latency)
        print(f"  [{i+1}/{ITERATIONS}] {label} {response.latency:.3f}s")

    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’é™¤å¤–ã—ã¦çµ±è¨ˆè¨ˆç®—
    valid_latencies = latencies[1:]
    avg = statistics.mean(valid_latencies)
    std = statistics.stdev(valid_latencies)

    print(f"\n  Results (Runs 2-4):")
    print(f"    Average: {avg:.3f}s")
    print(f"    Std Dev: {std:.3f}s")
    print(f"    Min/Max: {min(valid_latencies):.3f}s / {max(valid_latencies):.3f}s")

# å®Ÿè¡Œ
benchmark_provider(OllamaProvider(model="qwen2.5:3b"), "Ollama 3B")
benchmark_provider(OpenAIProvider(model="gpt-4o-mini"), "OpenAI gpt-4o-mini")
benchmark_provider(GeminiProvider(model="gemini-2.5-flash"), "Gemini 2.5 Flash")
```

---

# ğŸ“Š ã¾ã¨ã‚

| è¦³ç‚¹ | Ollama | OpenAI | Gemini |
|------|--------|--------|--------|
| **é€Ÿåº¦** | ğŸ¥‡ æœ€é€Ÿï¼ˆ0.368ç§’ï¼‰ | ğŸ¥‰ é…ã„ï¼ˆ1.717ç§’ï¼‰ | ğŸ¥ˆ æ™®é€šï¼ˆ1.692ç§’ï¼‰ |
| **ã‚³ã‚¹ãƒˆ** | ğŸ¥‡ ç„¡æ–™ | ğŸ¥‰ é«˜ã„ï¼ˆ$0.000015ï¼‰ | ğŸ¥ˆ å®‰ã„ï¼ˆ$0.000003ï¼‰ |
| **ä¿¡é ¼æ€§** | ğŸ¥‡ 100% | ğŸ¥‡ 100% | âš ï¸ 75%ï¼ˆæ—¥æœ¬èªæ³¨æ„ï¼‰ |
| **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** | ğŸ¥‰ ã‚„ã‚„é¢å€’ï¼ˆGPUå¿…è¦ï¼‰ | ğŸ¥‡ ç°¡å˜ï¼ˆAPIã‚­ãƒ¼ã®ã¿ï¼‰ | ğŸ¥‡ ç°¡å˜ï¼ˆAPIã‚­ãƒ¼ã®ã¿ï¼‰ |
| **ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼** | ğŸ¥‡ å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ« | ğŸ¥‰ ã‚¯ãƒ©ã‚¦ãƒ‰é€ä¿¡ | ğŸ¥‰ ã‚¯ãƒ©ã‚¦ãƒ‰é€ä¿¡ |

## çµè«–

AI VTuberãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆ**ãŒæœ€é©ï¼š

1. **ãƒ¡ã‚¤ãƒ³**: Ollama qwen2.5:3bï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é…ä¿¡ã€ç„¡æ–™ï¼‰
2. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: Gemini 2.5 Flashï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å¯ç”¨æ€§ã€è¶…ä½ã‚³ã‚¹ãƒˆï¼‰
3. **ç‰¹æ®Šã‚±ãƒ¼ã‚¹**: OpenAI gpt-4o-miniï¼ˆå“è³ªæœ€é‡è¦–æ™‚ã®ã¿ï¼‰

ã“ã®æ§‹æˆã«ã‚ˆã‚Šï¼š
- âœ… **99%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›**ï¼ˆæœˆ$150 â†’ $0.15ï¼‰
- âœ… **æœ€é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹**ï¼ˆ0.368ç§’ï¼‰
- âœ… **é«˜å¯ç”¨æ€§**ï¼ˆ3æ®µéšãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

---

# ğŸ”— å‚è€ƒãƒªãƒ³ã‚¯

- [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªãƒã‚¸ãƒˆãƒª](https://github.com/koshikawa-masato/AI-Vtuber-Project)
- [Ollamaå…¬å¼](https://ollama.com/)
- [OpenAI API](https://platform.openai.com/)
- [Google Gemini API](https://ai.google.dev/)
- [ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆè©³ç´°ç‰ˆï¼‰](https://github.com/koshikawa-masato/AI-Vtuber-Project/tree/main/benchmarks)

---

# è‘—è€…æƒ…å ±

**koshikawa-masato**
- AI VTuberãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–‹ç™ºè€…
- 50å¹´ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢çµŒé¨“ï¼ˆãƒ‘ã‚½ã‚³ãƒ³é€šä¿¡æ™‚ä»£ã‹ã‚‰ï¼‰
- å®Œå…¨ãƒ­ãƒ¼ã‚«ãƒ«AI VTuberã‚·ã‚¹ãƒ†ãƒ ã®å…ˆé§†è€…

ã“ã®è¨˜äº‹ãŒå‚è€ƒã«ãªã£ãŸã‚‰ã€ãœã² **LGTM** ã‚’ãŠé¡˜ã„ã—ã¾ã™ï¼ğŸ™
