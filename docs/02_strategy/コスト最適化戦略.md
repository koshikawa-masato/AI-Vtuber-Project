# コスト最適化戦略（修正版）：VPS環境の現実を踏まえた設計

## 重要な前提の修正

### ❌ 誤った前提（初版）
```
VPS本番環境でOllamaを併用
  → VPSでローカルLLMを動かせる
```

### ✅ 現実
```
VPS環境（さくらVPS）:
  - CPU: 限定的
  - GPU: なし
  - メモリ: 2GB程度
  → Ollama（qwen2.5:32b）は動作不可能
  → クラウドLLM（OpenAI）のみ現実的
```

---

## 修正後のアーキテクチャ

### システム構成

```
┌─────────────────────────────────────────┐
│         VPS環境（本番）                   │
│  - LLM: OpenAI API（gpt-4o/gpt-4o-mini）│
│  - プロレスパターンDB参照                 │
│  - ルールベース応答                      │
└─────────────────────────────────────────┘
              │
              │ ログ収集（夜間）
              ▼
┌─────────────────────────────────────────┐
│      ローカル環境（学習専用）             │
│  - LLM: Ollama（qwen2.5:32b）           │
│  - 反実仮想シナリオ生成                  │
│  - プロレスパターン学習                  │
└─────────────────────────────────────────┘
```

**重要な変更点**:
- VPS本番 = OpenAI APIのみ（Ollama不使用）
- ローカル環境 = 学習・検証のみ（本番には関与しない）

---

## 修正後のコスト最適化戦略

### 戦略1: プロレスパターンDBの活用（最重要）

**基本方針**: LLM呼び出しを減らす

```python
class ResponseGenerator:
    def generate(self, user_message: str) -> str:
        # 1. プロレスパターンDBから検索
        if pattern := self.prowrestling_db.find_match(user_message):
            return pattern['response']  # LLM不要！コストゼロ
        
        # 2. ルールベース応答
        if simple_response := self.rule_based_response(user_message):
            return simple_response  # LLM不要！コストゼロ
        
        # 3. LLM呼び出し（最終手段）
        return self.llm.generate(user_message)  # コスト発生
```

#### コスト削減効果

```
Phase 1（初期）:
  パターンDB利用率: 10%
  ルールベース: 20%
  LLM呼び出し: 70%
  → コスト: ¥3,150/月（30%削減）

Phase 2（3ヶ月後）:
  パターンDB利用率: 40%
  ルールベース: 20%
  LLM呼び出し: 40%
  → コスト: ¥1,800/月（60%削減）

Phase 3（6ヶ月後）:
  パターンDB利用率: 60%
  ルールベース: 20%
  LLM呼び出し: 20%
  → コスト: ¥900/月（80%削減）
```

---

### 戦略2: ルールベース応答の強化

**問題**: すべての会話でLLMを使うのは非効率

**解決策**: 定型的な会話はルールで処理

```python
class RuleBasedResponder:
    """
    ルールベース応答（コストゼロ）
    """
    
    PATTERNS = {
        # 挨拶
        r"(おはよう|朝)": [
            "おはよー！今日も元気にいこ！",
            "おはよう！よく眠れた？",
        ],
        r"(こんにちは|昼)": [
            "こんにちは！何してた？",
            "やっほー！今日の調子どう？",
        ],
        
        # 感謝
        r"(ありがと|サンキュー)": [
            "いえいえ、どういたしまして！",
            "気にしないで～！",
        ],
        
        # 簡単な質問
        r"(元気|調子)": [
            "うん、元気だよ！あなたは？",
            "バッチリ！今日も絶好調！",
        ],
        
        # 名前
        r"(名前|誰)": [
            "私は牡丹！よろしくね！",
            "牡丹だよ！覚えてね～",
        ],
    }
    
    def respond(self, message: str, character: str) -> Optional[str]:
        """
        パターンマッチで応答
        """
        for pattern, responses in self.PATTERNS.items():
            if re.search(pattern, message):
                # キャラクターごとにカスタマイズ
                return self.customize_for_character(
                    random.choice(responses),
                    character
                )
        return None  # パターンマッチ失敗
```

#### コスト削減効果

```
ルールベース対応可能な会話: 約20-30%
  - 挨拶
  - 簡単な質問
  - 定型的な返事

削減コスト: ¥900-1,350/月
```

---

### 戦略3: gpt-4o-miniの積極活用

**gpt-4o vs gpt-4o-mini**:

```
gpt-4o:
  Input: $15/1M tokens ($0.015/1K tokens)
  Output: $60/1M tokens ($0.060/1K tokens)
  用途: 複雑な会話、プロレス、新規ユーザー

gpt-4o-mini:
  Input: $0.15/1M tokens ($0.00015/1K tokens)
  Output: $0.60/1M tokens ($0.00060/1K tokens)
  用途: 通常会話、簡単な質問
  
→ 100倍安い！
```

#### 実装

```python
class LLMRouter:
    """
    会話の重要度に応じてモデルを選択
    """
    
    def select_model(self, user_message: str, context: dict) -> str:
        importance = self.calculate_importance(user_message, context)
        
        if importance >= 0.7:
            return "gpt-4o"  # 重要な会話
        else:
            return "gpt-4o-mini"  # 通常会話
    
    def calculate_importance(self, message: str, context: dict) -> float:
        score = 0.0
        
        # 新規ユーザー（第一印象重要）
        if context.get('is_new_user'):
            score += 0.4
        
        # プロレス（じゃれ合い）→ 学習対象
        if context.get('layer4_judgment') == "じゃれ合い":
            score += 0.5
        
        # 長文（複雑な会話）
        if len(message) > 50:
            score += 0.2
        
        # キャラクター切り替え直後
        if context.get('just_switched_character'):
            score += 0.3
        
        # 定型挨拶は重要度低い
        if any(g in message for g in ["おはよう", "こんにちは", "おやすみ"]):
            score -= 0.5
        
        return min(1.0, max(0.0, score))
```

#### コスト削減効果

```
想定:
  重要会話（gpt-4o）: 30%
  通常会話（gpt-4o-mini）: 70%

コスト計算:
  gpt-4o部分: ¥4,500 * 0.3 = ¥1,350
  gpt-4o-mini部分: ¥45 * 0.7 = ¥31.5
  
合計: ¥1,381.5/月（69%削減）
```

---

### 戦略4: コンテキスト最適化

**問題**: 会話履歴が長くなるとトークン数が増大

**解決策**: 
1. 必要最小限の履歴のみ送信
2. 古い会話は要約（ローカルOllamaで事前処理）
3. キャラクター情報は圧縮

```python
class ContextOptimizer:
    """
    コンテキストを最適化してトークン数を削減
    """
    
    def optimize(self, conversation_history: list, character: str) -> dict:
        """
        最適化されたコンテキストを生成
        """
        # システムプロンプト（キャッシュ利用）
        system_prompt = self.get_cached_system_prompt(character)
        
        # 会話履歴（最新5件のみ）
        recent_messages = conversation_history[-5:]
        
        # 古い会話の要約（ローカル環境で事前生成）
        if len(conversation_history) > 5:
            summary = self.get_pregenerated_summary(conversation_history[:-5])
        else:
            summary = None
        
        return {
            'system': system_prompt,
            'summary': summary,
            'recent': recent_messages
        }
    
    def get_cached_system_prompt(self, character: str) -> str:
        """
        キャッシュ可能な形式でシステムプロンプトを返す
        """
        # OpenAIのPrompt Cachingを活用
        return SYSTEM_PROMPTS[character]  # 固定プロンプト
```

#### コスト削減効果

```
従来:
  システムプロンプト: 500 tokens（毎回送信）
  会話履歴全て: 平均300 tokens
  合計Input: 800 tokens/会話

最適化後:
  システムプロンプト: 50 tokens（キャッシュから）
  会話履歴最新5件: 100 tokens
  合計Input: 150 tokens/会話

削減率: 81%
Input costの削減: ¥11.25 → ¥2.06/月
```

---

### 戦略5: プロンプト最適化

**問題**: 冗長なプロンプトでトークンを浪費

**解決策**: 簡潔で効果的なプロンプト

```python
# ❌ 冗長なプロンプト（500 tokens）
"""
あなたは牡丹という17歳の女性です。
性格は明るくて社交的で、LA帰りの帰国子女です。
趣味はファッションと音楽で、特にK-POPが好きです。
話し方はギャル風で、「～じゃん」「～だし」などの語尾を使います。
ユーザーとは友達のように接してください。
プロレス（じゃれ合い）が得意で、ユーモアを交えて返答します。
...（長い説明が続く）
"""

# ✅ 簡潔なプロンプト（150 tokens）
"""
牡丹（17歳、次女、LA帰り）
性格: 明るい、社交的、負けず嫌い
口調: ギャル風（～じゃん、～だし）
得意: プロレス（じゃれ合い）、ユーモア
"""
```

#### コスト削減効果

```
プロンプト最適化:
  500 tokens → 150 tokens（70%削減）
  
月間削減:
  (500-150) × 50会話/日 × 30日 = 525,000 tokens/月
  コスト削減: ¥7.88/月
```

---

### 戦略6: キャッシュの徹底活用

**OpenAI Prompt Caching**:

```python
# システムプロンプトをキャッシュ
# 1時間以内の再利用で大幅にコスト削減

response = openai.chat.completions.create(
    model="gpt-4o",
    messages=[
        {
            "role": "system",
            "content": SYSTEM_PROMPT,  # ← これがキャッシュされる
        },
        {
            "role": "user",
            "content": user_message
        }
    ],
    # キャッシュは自動的に有効化される
)
```

#### コスト削減効果

```
システムプロンプト: 150 tokens

従来:
  150 tokens × 50会話/日 = 7,500 tokens/日
  月間: 225,000 tokens
  コスト: ¥3.38/月

キャッシュ利用:
  初回: 150 tokens（フル料金）
  2回目以降（1時間以内）: 15 tokens（10%の料金）
  
平均: 30 tokens/会話（80%削減）
月間コスト: ¥0.68/月

削減: ¥2.70/月
```

---

## 総合コスト試算（修正版）

### ベースライン

```
全会話でgpt-4oを使用:
  50会話/日 × 30日 = 1,500会話/月
  
Input: 500 tokens/会話 × 1,500 = 750,000 tokens
  → ¥11.25/月
  
Output: 200 tokens/会話 × 1,500 = 300,000 tokens
  → ¥18.00/月
  
合計: ¥29.25/月 ≈ ¥4,500/月（$1=¥150）
```

### 最適化後（すべての戦略を適用）

```
Phase 1（即座に実施可能）:

1. プロレスパターンDB: 10%削減
2. ルールベース: 20%削減
3. gpt-4o-mini: 70%の会話で使用
4. コンテキスト最適化: Input 81%削減
5. プロンプト最適化: 70%削減
6. キャッシュ活用: 80%削減

計算:
  パターンDB/ルールベース: 30% → LLM不使用
  
  残り70%のLLM使用:
    - 30%: gpt-4o（重要会話）
    - 70%: gpt-4o-mini（通常会話）
  
  gpt-4o部分:
    Input: 750,000 × 0.3 × 0.7 × 0.19 × 0.3 × 0.2 = 1,784 tokens
    Output: 300,000 × 0.3 × 0.7 = 63,000 tokens
    コスト: ¥0.03 + ¥3.78 = ¥3.81
  
  gpt-4o-mini部分:
    Input: 750,000 × 0.7 × 0.7 × 0.19 × 0.3 × 0.2 = 4,162 tokens
    Output: 300,000 × 0.7 × 0.7 = 147,000 tokens
    コスト: ¥0.0006 + ¥0.088 = ¥0.089

合計: ¥3.81 + ¥0.089 ≈ ¥4/月

削減率: 99%以上！
```

**実際には**: ¥500-800/月程度が現実的
- 新規ユーザーの増加
- 予期しない複雑な会話
- バッファとして

---

## 実装優先順位（修正版）

### Phase 1: 即座に実施（今週）

1. **gpt-4o-miniへの切り替え**
   ```python
   # src/line_bot_vps/cloud_llm_provider.py
   def select_model(self, importance: float) -> str:
       return "gpt-4o" if importance >= 0.7 else "gpt-4o-mini"
   ```
   **効果**: 70%削減 → ¥1,350/月

2. **ルールベース応答の追加**
   ```python
   # 挨拶、簡単な質問に対応
   ```
   **効果**: さらに20%削減 → ¥1,080/月

**即座の効果**: ¥4,500 → ¥1,080/月（76%削減）

### Phase 2: 2週間以内

3. **コンテキスト最適化**
   ```python
   # 会話履歴を最新5件に制限
   # プロンプトの簡潔化
   ```
   **効果**: さらに40%削減 → ¥648/月

4. **プロンプトキャッシュの確認**
   ```python
   # OpenAI APIで自動的に有効
   ```
   **効果**: さらに20%削減 → ¥518/月

### Phase 3: 1ヶ月以内

5. **プロレスパターンDBの活用**
   ```python
   # 学習済みパターンから応答
   ```
   **効果**: Phase毎に10-20%削減

**最終目標**: ¥500-800/月（89%削減）

---

## モニタリング

### コスト追跡

```python
class CostTracker:
    def log_usage(self, model: str, input_tokens: int, output_tokens: int):
        cost = self.calculate_cost(model, input_tokens, output_tokens)
        
        self.daily_cost += cost
        self.monthly_cost += cost
        
        # 予算チェック
        if self.monthly_cost > BUDGET:
            self.send_alert(f"⚠️ 予算超過: ¥{self.monthly_cost}")
    
    def calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        if model == "gpt-4o":
            return (input_tokens / 1_000_000 * 15 + 
                    output_tokens / 1_000_000 * 60) * 150  # ¥換算
        elif model == "gpt-4o-mini":
            return (input_tokens / 1_000_000 * 0.15 + 
                    output_tokens / 1_000_000 * 0.60) * 150
        else:
            return 0  # Pattern DB / Rule-based
```

---

## まとめ

### VPS環境の現実

- ❌ Ollama（ローカルLLM）は動作不可
- ✅ OpenAI APIのみ使用
- ✅ プロレスパターンDBで補完

### 修正後の最適化戦略

1. **gpt-4o-mini活用**: 70%の会話で使用 → 大幅削減
2. **ルールベース応答**: 20-30%の会話をカバー
3. **プロレスパターンDB**: 学習が進むほど増加
4. **コンテキスト最適化**: トークン数を81%削減
5. **プロンプト最適化**: 簡潔で効果的なプロンプト
6. **キャッシュ活用**: システムプロンプトを再利用

### コスト削減目標

```
Phase 1（即座）: ¥1,080/月（76%削減）
Phase 2（2週間）: ¥518/月（88%削減）
Phase 3（1ヶ月）: ¥500-800/月（89%削減）
長期（6ヶ月）: ¥300-500/月（93%削減）
```

**親の財布を守りつつ、娘たちの品質は維持する。**

---

**🤖 Generated with Claude Code**

**Co-Authored-By**: Claude <noreply@anthropic.com>

**作成日**: 2025-11-13（修正版）
